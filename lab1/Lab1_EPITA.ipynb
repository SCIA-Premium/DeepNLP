{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMwkByBnaE49"
      },
      "source": [
        "<p style=\"text-align:center;\"><img src=\"data:image/jpeg;base64,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\" alt=\"EPITA Lab 1\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKtcL0CYn2n_"
      },
      "source": [
        "# Part 1. Keywords Extraction (14 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXcmrzodG1pt"
      },
      "source": [
        "## What is Keyword Extraction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZWKLXCYG9NJ"
      },
      "source": [
        "Keyword extraction is defined as the task that automatically identifies a set of the terms that best describe the subject of document. This is an important method in information retrieval (IR) systems: keywords simplify and speed up the search. Keyword extraction can be used to reduce the dimensionality of text for further text analysis (text classification ot topic modeling). S.Art et al., for example, extracted keywords to measure patent similarity. Using keyword extraction, you can automatically index data, summarize a text, or generate tag clouds with the most representative keywords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O3FI910HETW"
      },
      "source": [
        "## How to extract the keywords?\n",
        "All keyword extraction algorithms include the following steps:\n",
        "\n",
        "* Candidate generation. Detection of possible candidate keywords from the text.\n",
        "* Property calculation. Computation of properties and statistics required for ranking.\n",
        "* Ranking. Computation of a score for each candidate keyword and sorting in descending order of all candidates. The top n candidates are finally selected as the n keywords representing the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wD64EYyAHE5k"
      },
      "outputs": [],
      "source": [
        "# all the imports \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr3WHRR0C9Il"
      },
      "source": [
        "## Goal.\n",
        "\n",
        "In the following, given a paper, we will extract the keywords associated to this paper. Each individual can have their own qualitative assessment of what is \"key\" word. However, we will try as much as possible to objectify the approach and quantify to what extent a keyword is indeed key to the paper in question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoUQe5Df4Bpr"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xT3cUUST9QX6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! git clone https://github.com/MastafaF/ExtractKeywords.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duJTpCLQ9XJ5",
        "outputId": "c6f6843c-4587-4c74-f53e-9940e6612947"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.git', 'README.md', 'data.tar.gz', 'LICENSE']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os \n",
        "\n",
        "os.listdir(\"./ExtractKeywords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfVjwTfE9dyP",
        "outputId": "38b79a0a-96c7-4299-a8fe-e521edc51868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/\n",
            "data/papers.csv\n"
          ]
        }
      ],
      "source": [
        "# Extract data file \n",
        "\n",
        "! cd ExtractKeywords && tar -zxvf data.tar.gz data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CdRVtKw44C9p",
        "outputId": "7043799f-5a23-48f7-b3bb-44cf187d10c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-40439386-608d-48aa-82a3-ebfa4f2ddd7a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40439386-608d-48aa-82a3-ebfa4f2ddd7a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40439386-608d-48aa-82a3-ebfa4f2ddd7a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40439386-608d-48aa-82a3-ebfa4f2ddd7a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('./ExtractKeywords/data/papers.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2IElpuJ76o0"
      },
      "source": [
        "## Preprocessing data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRAj0JXjAWBo",
        "outputId": "e0a9ee71-8140-40f1-98b9-827c4438c189"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For the Lemmatizer \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHC9ShM-IX7E"
      },
      "source": [
        "### Question 1.1: Preprocessing data in a meaningful way [code] (2 points)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BWpI1OJk78Gc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "# Update stop words accordingly\n",
        "#my_stop_words = STOPWORDS.union(set(['mystopword1', 'mystopword2']))\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
        "             \"show\", \"result\", \"large\", \n",
        "             \"also\", \"one\", \"two\", \"three\", \n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
        "\n",
        "stop_words = STOPWORDS.union(set(new_words))\n",
        "\n",
        "def pre_process(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "\n",
        "    # Convert to list from string\n",
        "    text = text.split()\n",
        "\n",
        "    #Lemmatisation\n",
        "    lem = WordNetLemmatizer()\n",
        "    text = [lem.lemmatize(word) for word in text if not word in  \n",
        "            stop_words] \n",
        "    text = \" \".join(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZFsz9JX_9I0",
        "outputId": "e46cb548-74eb-45a8-a70f-05b399c6828e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 35s, sys: 341 ms, total: 1min 35s\n",
            "Wall time: 1min 40s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df['preproc_text'] = df['paper_text'].apply(pre_process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "_3ADgBBsAHhg",
        "outputId": "1fc3d72a-17ab-4925-cf7f-00e7acb8a717"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>preproc_text</th>\n",
              "      <td>self organization associative database application hisashi suzuki suguru arimoto osaka university toyonaka osaka japan abstract efficient method self organizing associative database proposed application robot eyesight system proposed database associate input output half discussion algorithm self organization proposed aspect hardware produce new style neural network half applicability handwritten letter recognition autonomous mobile robot demonstrated introduction let mapping f x y given x finite infinite set y finite infinite set learning machine observes set pair x y sampled randomly x x y x x y mean cartesian product x y computes estimate j x y f small estimation error measure usually faster decrease estimation error increase number sample better learning machine expression performance incomplete lack consideration candidate j j assumed preliminarily good learning machine clarify conception let discus type learning machine let advance understanding self organization associative database parameter type ordinary type learning machine assumes equation relating x s y s parameter indefinite structure f equivalent define implicitly set f candidate f subset mapping x y computes value parameter based observed sample type parameter type learning machine defined f f j approach f number sample increase alternative case estimation error remains eternally problem designing learning machine return proper structure f sense hand assumed structure f demanded compact possible achieve fast learning word number parameter small parameter j uniquely determined observed sample demand proper contradicts compact consequently parameter type better compactness assumed structure proper better learning machine elementary conception design learning machine universality ordinary neural network suppose sufficient knowledge f given j unknown case comparatively easy proper compact structure j alternative case difficult possible solution compactness assume almighty structure cover s combination orthogonal base infinite dimension structure neural network approximation obtained truncating finitely dimension implementation american institute physic main topic designing neural network establish desirable structure work includes developing practical procedure compute value coefficient observed sample discussion flourishing efficient method proposed recently hardware unit computing coefficient parallel speed sold e g anza mark iii odyssey e neural network exists danger error remaining eternally estimating precisely speaking suppose combination base finite number define structure essentially word suppose f located near f case estimation error negligible distant f estimation error negligible research report following situation appears complex estimation error converges value number sample increase decrease hardly dimension heighten property considerable defect neural network recursi ve type recursive type founded methodology learning follows initial stage set fa instead notation f candidate equal set mapping x y observing xl yl e x x y fa reduced fi xt yl e f observing second x y e x x y fl reduced f xt yl x y e f candidate set f gradually small observation sample proceeds observing sample write likelihood estimation selected fi contrarily parameter type recursive type guarantee surely j approach number sample increase recursive type observes x yd rewrite value l x s x s x s correlated type architecture composed rule rewriting free memory space architecture form naturally kind database build management system data self organizing way database differs ordinary one following sense record sample observed computes estimation l x x e x database associative database subject constructing associative database establish rule rewri ting purpose adap t measure called dissimilari ty dissimilari ty mean mapping d x x x real o x x e x x x d x x l x t x necessarily defined single formula definable example collection rule written form dissimilarity d defines structure locally x x y knowledge f imperfect flect d heuristic way contrarily neural network possible accelerate speed learning establishing d especially easily simple d s l s process analogically information like human application paper s recursive type show strongly effectiveness denote sequence observed sample xl yd x y simplest construction associative database observing sample follows algorithm initial stage let set let l x x e x equal y x y e s l d x x min y e t d x x furthermore add x y s l produce sa e s s l u x y n version improved economize memory follows algorithm initial stage let composed arbitrary element x x y let ii lex x e x equal y x y e si l d x x min d x x e l furthermore ii l xi yi let si si l add xi yi si l produce si e si si l u xi yi construction ii approach f increase computation time grows proportionally size si second subject constructing associative database addressing rule employ economize computation time subsequent chapter construction associative database purpose proposed manages data form binary tree self organization associative database given sequence xl yl x y algorithm constructing associative database follows algorithm step initialization let x root y root xl yd x y variable assigned respective node memorize data furthermore let t step increase t x reset pointer n root repeat following n arrives terminal node e leaf notation nand d xt x n let n n mean descendant node n n let n n d x r n step display yin related information y yin y step establish new descendant node n n secondly let x n yin x n yin x n yin xt y finally step loop step stopped time continued suppose gate element artificial synapsis play role branching d prepared obtain new style neural network gate element randomly connected algorithm letter recognition recen tly vertical slitting method recognizing typographic english letter elastic matching method recognizing hand written discrete english letter global training fuzzy logic search method recognizing chinese character written square style published self organization associative database realizes recognition handwritten continuous english letter wn nov xk la t dw lo source document loo h o o windowing number sample o nualber sampl e experiment scanner take document letter recognizer us parallelogram window cover maximal letter process sequence letter shifting window recognizer scan word slant direction place window left vicinity black point detected window catch letter succeeding letter recognition head letter performed end position boundary line letter known starting scanning boundary repeating operation recognizer accomplishes recursively task major problem come identifying head letter window considering define following regard window image x s define x accordingly x x e x x x denote b black point left area boundary window x project b window x measure euclidean distance fj black point b x closest b let d x x summation s black point b s x divided number b s regard couple reading position boundary y s define y accordingly operator teach recognizer interaction relation window reading boundary algorithm precisely recalled reading incorrect operator teach correct reading console boundary position incorrect teach correct position mouse show partially document experiment show change number node recognition rate defined relative frequency correct answer past trial speciiications window height dot width dot slant angular deg example level tree distributed time recognition rate converged experimentally recognition rate converges case rare case attain e g c e distinguishable excessive lluctuation writing consistency x y relation assured like number node increase endlessly d clever stop learning recognition rate attains upper limit improve recognition rate consider spelling word future subject obstacle avoiding movement system camera type autonomous mobile robot reported flourishingly o author belongs category mathematical methodology solve usually problem obstacle avoiding movement cost minimization problem cost criterion established artificially contrarily self organization associative database reproduces faithfully cost criterion operator motion robot learning natural length width height robot o m weight visual angle camera deg robot following factor motion turn deg advance m control speed h experiment passageway wid th m inside building author laboratory exist experimental intention arrange box smoking stand gas cylinder stool handcart passage way random let robot camera recall similar trace route preliminarily recorded purpose define following let camera face deg downward process low pas filter scanning vertically filtered search point c luminance change excessively su bstitu te point c white point c black obstacle exists robot white area show free area robot regard binary x dot image processed x s define x accordingly x x e x x x let d x x number black point exclusive x x regard y s image obtained drawing route image x s define y accordingly robot superimposes current camera x route recalled x inquires operator instruction operator judge subjectively suggested route appropriate negative answer draw desirable route x mouse teach new y robot opera tion defines implicitly sequence x y reflecting cost criterion operator l iibube roan stationary uni t configuration autonomous mobile robot north rmbi unit robot roan y t experimental environment wall camera preprocessing fa preprocessing o course suggest ion search processing obstacle avoiding movement x processing position identification define satisfaction rate relative frequency acceptable suggestion route past trial typical experiment change satisfaction rate showed similar tendency attains time notice rest mean directly percentage collision practice prevent collision adopting supplementary measure time number node level tree distributed proposed method reflects delicately character operator example robot trained operator move slowly space obstacle trained operator brush quickly obstacle fact give hint method printing character machine position identification robot identify position recalling similar landscape position data camera purpose principle suffices regard camera image position data x s y s respectively memory capacity finite actual compu ters compress camera image slight loss information compression admittable long precision position identification acceptable area major problem come suitable compression method experimental environment jut passageway interval m section adjacent jut door robot identifies roughly surrounding landscape section place us temporarily triangular surveying technique exact measure necessary realize task define following turn camera panorama deg scanning horizontally center line substitute point luminance excessively change black point white regard binary dot line image processed x s define x accordingly x x e x x x project black point x x measure euclidean distance black point x closest let summation s similarly calculate s exchanging role x x denoting number s s respectively nand n define d x x n n regard positive integer labeled section y s cf define y accordingly learning mode robot check exactly position counter reset periodically operator robot run arbitrarily passageway m area learns relation landscape position data position identification m area achieved crossing plural database task automatic excepting periodic reset counter kind learning teacher define identification rate relative frequency correct recall position data past trial typical example converged time time number level level oftree distributed identification failure rejected considering trajectory pro blem arises practical use order improve identification rate compression ratio camera image loosened possibility depends improvement hardware future show example actual motion robot based database obstacle avoiding movement position identification example corresponds case moving time interval frame sec t ii actual motion robot conclusion method self organizing associative database proposed application robot eyesight system machine decomposes global structure unknown set local structure known learns universally input output response framework problem implies wide application area example shown paper defect algorithm self organization tree balanced subclass structure f subject imposed widen class probable solution abolish addressing rule depending directly value d instead establish rule depending distribution function value d investigation reference hopfield j j d w tank computing neural circuit model science pp rumelhart d e et al learning representation propagating error nature pp hull j j hypothesis generation computational model visual word recognition ieee expert fall pp kurtzberg j m feature analysis symbol recognition elastic matching ibm j re develop pp wang q r c y suen tree classifier heuristic search global training ieee trans pattern anal mach intell pami pp brook r et al self calibration motion stereo vision mobile robot th int symp robotics research pp goto y stentz cmu mobile robot navigation ieee int conf robotics automation pp madarasz r et al design autonomous vehicle disabled ieee jour robotics automation ra pp triendl e d j kriegman stereo vision navigation building ieee int conf robotics automation pp turk m et al video road following autonomous land vehicle ieee int conf robotics automation pp</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualizing data \n",
        "HTML(pd.DataFrame(df.loc[0, [\"preproc_text\"]]).to_html())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLS9mbe8B4UY"
      },
      "source": [
        "## 0. Raw counts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WY9c6iwI0Lv"
      },
      "source": [
        "### Question 1.2: Build a top N words based on occurence [code] (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izLhCAH5B-jy",
        "outputId": "9179942d-8590-4d33-c2b5-e2758c22fa10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Idea: \n",
        "\n",
        "0. Split with spacy OR nltk \n",
        "\n",
        "1. Counter \n",
        "\n",
        "2. Surface top 10 \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def get_counter(txt_preproc, N=10): \n",
        "    # Use nltk to split the text\n",
        "    tokens = nltk.word_tokenize(txt_preproc)\n",
        "\n",
        "    # Create a counter\n",
        "    counter = Counter(tokens)\n",
        "\n",
        "    # Get the top N words\n",
        "    top_N = counter.most_common(N)\n",
        "\n",
        "    return top_N\n",
        "\n",
        "df[\"Top N\"] = df[\"preproc_text\"].apply(get_counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-J0RHMMFW6b",
        "outputId": "93de2f37-e65f-4eb0-ccea-58528d37169a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('input', 58),\n",
              " ('p', 42),\n",
              " ('weak', 42),\n",
              " ('s', 40),\n",
              " ('synaptic', 36),\n",
              " ('associative', 35),\n",
              " ('ltp', 30),\n",
              " ('strong', 26),\n",
              " ('phase', 26),\n",
              " ('long', 24)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[2, \"Top N\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK7YbNCyJaqk"
      },
      "source": [
        "### Question 1.3: What are some of the limits of raw counts? How could we improve the approach through preprocessing? [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eewWd_TjJlt-"
      },
      "source": [
        "Some of the limits of raw counts for top-N words based on occurence include\n",
        "the following:\n",
        "\n",
        "• Raw counts do not take into account the context in which words are used, so\n",
        "words with multiple meanings may be counted incorrectly.\n",
        "\n",
        "• Raw counts also do not account for different forms of a word (e.g. plural vs.\n",
        "singular), so words that are inflected differently may be counted differently.\n",
        "\n",
        "• Raw counts may be biased towards longer documents, as shorter documents\n",
        "will have fewer total words and thus a lower chance of containing the top-N\n",
        "words.\n",
        "\n",
        "• Finally, raw counts do not account for the fact that some words are more\n",
        "common than others and may therefore be over-represented in the top-N list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wao4NivXGPIM"
      },
      "source": [
        "## 1. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TRcD4MeHFnt"
      },
      "source": [
        "### Introduction.\n",
        "\n",
        "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases proportionally to the number of times a word appears in the document (Text Frequency - TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency - IDF). Using the tf-idf weighting scheme, the keywords are the words with the higherst TF-IDF score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOZe8obbHm-Q"
      },
      "source": [
        "### CountVectorizer to create a vocabulary and generate word counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pd54cOkGV_p",
        "outputId": "da09b01b-2ce9-448e-c190-c4187ad165e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2min, sys: 3.81 s, total: 2min 3s\n",
            "Wall time: 2min 4s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "#create a vocabulary of words, \n",
        "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
        "                   max_features=10000,  # the size of the vocabulary\n",
        "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
        "                  )\n",
        "\n",
        "\n",
        "word_count_vector=cv.fit_transform(df[\"preproc_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fngZVCVGcG_",
        "outputId": "9db7fdef-c21d-4152-e9ff-c1dc4a128833"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<7241x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 5744173 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_count_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxV_evcXHuKp"
      },
      "source": [
        "### TfidfTransformer to Compute Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCfwFzHCHzOY",
        "outputId": "6ef1561d-9838-4a54-ba4f-40917db30d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 21.3 ms, sys: 1.97 ms, total: 23.3 ms\n",
            "Wall time: 25.4 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,\n",
        "                                   use_idf=True)\n",
        "\n",
        "tfidf_transformer.fit(word_count_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd9hXGWJHzLy",
        "outputId": "00084835-bd24-4cb7-9392-6e5724fcab7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1wbWizrJ5CE"
      },
      "source": [
        "### Question 1.4: How can you find an optimal max_df? Why are we using a sparse matrix instead of a regular matrix? [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EekhsGbKYoyv"
      },
      "source": [
        "There is no definitive answer for this question, as it depends on the data and the desired outcome. However, one method to find an optimal max_df would be to experiment with different values and see which one results in the best performance for the specific task at hand. Another method would be to use a grid search to exhaustively search for the best max_df value.\n",
        "\n",
        "\n",
        "A sparse matrix is used in the TF-IDF algorithm because it is more efficient than a regular matrix when dealing with large amounts of data. A sparse matrix is a matrix that has a small number of non-zero elements. This means that the algorithm can run faster and use less memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_9miFxKaU1",
        "outputId": "d663fbc4-8ed2-42ab-fa69-5676b9e09767"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 10 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv.transform([\" change number node recognition rate defined relative frequency\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "K2Vdn9NTMIxI",
        "outputId": "bd5f8323-a684-4ee7-b9e1-5221b0405557"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7f1d0e479640>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualizing data \n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "plt.spy(csr_matrix(cv.transform([\"change number node recognition rate defined relative frequency\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRI_AZtROxP0",
        "outputId": "6b801aed-17f1-4163-f9b9-a102d583e01c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(7273, 0.6386357955896566),\n",
              " (6026, 0.4825597544042752),\n",
              " (3207, 0.28261086976733),\n",
              " (5886, 0.266976586175402),\n",
              " (7265, 0.22854270978240254),\n",
              " (7395, 0.22167942880961272),\n",
              " (1160, 0.1919695855420322),\n",
              " (7189, 0.17644900652980194),\n",
              " (2001, 0.15313791684291167),\n",
              " (6004, 0.12378880457769045)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([\"change number node recognition rate defined relative frequency\"]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "sorted_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxhq2LiEOxD3",
        "outputId": "3b9cbb5f-db7a-4d35-bc4a-c46d055e0d4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 10 stored elements in COOrdinate format>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coo_matrix = tf_idf_vector.tocoo()\n",
        "# list(zip(coo_matrix.col, coo_matrix.data))\n",
        "coo_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4EwdqH_Gm0C",
        "outputId": "78855f69-b08c-46a0-efbc-90aba8cc6d06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "def get_keywords(txt, top_N=10):\n",
        "    # get top N keywords score for the text and return a list of tuples (keyword, score)\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([txt]))\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "    sorted_keyword_score=[]\n",
        "    for idx, score in sorted_items[:top_N]:\n",
        "        # round the score to 3 decimal places\n",
        "        sorted_keyword_score.append((feature_names[idx], round(score, 3)))\n",
        "      \n",
        "    return sorted_keyword_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahvTo9gjSWCW",
        "outputId": "45b061e0-845f-42e6-fc37-fa874c600bfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('recognition rate', 0.639),\n",
              " ('number node', 0.483),\n",
              " ('frequency', 0.283),\n",
              " ('node', 0.267),\n",
              " ('recognition', 0.229),\n",
              " ('relative', 0.222),\n",
              " ('change', 0.192),\n",
              " ('rate', 0.176),\n",
              " ('defined', 0.153),\n",
              " ('number', 0.124)]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_keywords(txt=\"change number node recognition rate defined relative frequency\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9hzsg0CUQZ_"
      },
      "source": [
        "### Compare Raw Counts to Tf-IDF approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ot6GLVIiUPqj"
      },
      "outputs": [],
      "source": [
        "df[\"Top_N_TF-IDF\"] = df[\"preproc_text\"].apply(get_keywords, top_N=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "g4sydV20UjvA",
        "outputId": "36d3b834-57f7-4baa-81a6-b75a854ce96c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-90cbfb8c-da8c-4bc4-8e87-23da8d43e40e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "      <th>Top_N_TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5183</th>\n",
              "      <td>5692</td>\n",
              "      <td>2015</td>\n",
              "      <td>Bandits with Unobserved Confounders: A Causal ...</td>\n",
              "      <td>Poster</td>\n",
              "      <td>5692-bandits-with-unobserved-confounders-a-cau...</td>\n",
              "      <td>The Multi-Armed Bandit problem constitutes an ...</td>\n",
              "      <td>Bandits with Unobserved Confounders:\\nA Causal...</td>\n",
              "      <td>bandit unobserved confounders causal approach ...</td>\n",
              "      <td>[(x, 93), (s, 58), (bandit, 44), (y, 44), (arm...</td>\n",
              "      <td>[(bandit, 0.387), (arm, 0.347), (player, 0.258...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90cbfb8c-da8c-4bc4-8e87-23da8d43e40e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90cbfb8c-da8c-4bc4-8e87-23da8d43e40e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90cbfb8c-da8c-4bc4-8e87-23da8d43e40e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id  year                                              title  \\\n",
              "5183  5692  2015  Bandits with Unobserved Confounders: A Causal ...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "5183     Poster  5692-bandits-with-unobserved-confounders-a-cau...   \n",
              "\n",
              "                                               abstract  \\\n",
              "5183  The Multi-Armed Bandit problem constitutes an ...   \n",
              "\n",
              "                                             paper_text  \\\n",
              "5183  Bandits with Unobserved Confounders:\\nA Causal...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "5183  bandit unobserved confounders causal approach ...   \n",
              "\n",
              "                                                  Top N  \\\n",
              "5183  [(x, 93), (s, 58), (bandit, 44), (y, 44), (arm...   \n",
              "\n",
              "                                           Top_N_TF-IDF  \n",
              "5183  [(bandit, 0.387), (arm, 0.347), (player, 0.258...  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkDLV8yILNNS"
      },
      "source": [
        "### Question 1.5: Find an example where there is a noticeable difference between tf-idf and raw counts? Justify which method you would choose yourself (there is no bad and good answer here) [written] (2 points)\n",
        "\n",
        "\n",
        "There is no one-size-fits-all answer to this question, as the best method to use will depend on the specific data and task at hand. However, in general, I would tend to favor using tf-idf over raw counts, as tf-idf can help to downweight common words that are less informative for a particular task. This can be especially helpful when working with large data sets, as it can help to reduce the dimensionality of the data and improve the performance of machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mes_RnLNVXBX"
      },
      "source": [
        "## 2. KeyBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7iWThDYXlJY"
      },
      "source": [
        "## 2.0. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "g0Ji149nXmUU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "pip install keybert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "k-kXWUGKXqIB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from keybert import KeyBERT\n",
        "\n",
        "doc = \"\"\"\n",
        "         Supervised learning is the machine learning task of learning a function that\n",
        "         maps an input to an output based on example input-output pairs. It infers a\n",
        "         function from labeled training data consisting of a set of training examples.\n",
        "         In supervised learning, each example is a pair consisting of an input object\n",
        "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
        "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
        "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
        "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
        "         the learning algorithm to generalize from the training data to unseen situations in a \n",
        "         'reasonable' way (see inductive bias).\n",
        "      \"\"\"\n",
        "kw_model = KeyBERT()\n",
        "keywords = kw_model.extract_keywords(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNrYae2jYDqU",
        "outputId": "a9f50243-3be7-42c3-a87e-23c177bbdd8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('supervised', 0.6676),\n",
              " ('labeled', 0.4896),\n",
              " ('learning', 0.4813),\n",
              " ('training', 0.4134),\n",
              " ('labels', 0.3947)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJs_f30AYmKN"
      },
      "source": [
        "### Question 2.0. Apply KeyBERT to the a sample of the dataset [code] (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "7GisGAL0pIDy",
        "outputId": "2ef9c9e6-cd84-417c-9838-c5d26331a80c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2af41bdd-6278-4887-a3c0-5ce5215ad271\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "      <th>Top_N_TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2446</th>\n",
              "      <td>3219</td>\n",
              "      <td>2007</td>\n",
              "      <td>Active Preference Learning with Discrete Choic...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3219-active-preference-learning-with-discrete-...</td>\n",
              "      <td>We propose an active learning algorithm that l...</td>\n",
              "      <td>Active Preference Learning with Discrete Choic...</td>\n",
              "      <td>active preference learning discrete choice dat...</td>\n",
              "      <td>[(function, 55), (model, 49), (x, 49), (f, 44)...</td>\n",
              "      <td>[(valuation, 0.494), (user, 0.304), (preferenc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2af41bdd-6278-4887-a3c0-5ce5215ad271')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2af41bdd-6278-4887-a3c0-5ce5215ad271 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2af41bdd-6278-4887-a3c0-5ce5215ad271');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id  year                                              title  \\\n",
              "2446  3219  2007  Active Preference Learning with Discrete Choic...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "2446        NaN  3219-active-preference-learning-with-discrete-...   \n",
              "\n",
              "                                               abstract  \\\n",
              "2446  We propose an active learning algorithm that l...   \n",
              "\n",
              "                                             paper_text  \\\n",
              "2446  Active Preference Learning with Discrete Choic...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "2446  active preference learning discrete choice dat...   \n",
              "\n",
              "                                                  Top N  \\\n",
              "2446  [(function, 55), (model, 49), (x, 49), (f, 44)...   \n",
              "\n",
              "                                           Top_N_TF-IDF  \n",
              "2446  [(valuation, 0.494), (user, 0.304), (preferenc...  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ = df.sample(100)\n",
        "df_.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "CN3Dlu11YwLE",
        "outputId": "c4488c6b-8896-4c2e-d186-f4c0e13f791f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 24.1 s, sys: 8.73 s, total: 32.9 s\n",
            "Wall time: 26 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-908b8edf-fbc6-40a9-841d-05f7cf17e05a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "      <th>Top_N_TF-IDF</th>\n",
              "      <th>Top_N_KeyBERT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>2544</td>\n",
              "      <td>2004</td>\n",
              "      <td>A Large Deviation Bound for the Area Under the...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2544-a-large-deviation-bound-for-the-area-unde...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>A Large Deviation Bound\\nfor the Area Under th...</td>\n",
              "      <td>deviation bound area roc curve shivani agarwal...</td>\n",
              "      <td>[(y, 118), (f, 95), (x, 84), (n, 77), (ranking...</td>\n",
              "      <td>[(ranking, 0.479), (ranking function, 0.448), ...</td>\n",
              "      <td>[(ranking, 0.4367), (classification, 0.4331), ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-908b8edf-fbc6-40a9-841d-05f7cf17e05a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-908b8edf-fbc6-40a9-841d-05f7cf17e05a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-908b8edf-fbc6-40a9-841d-05f7cf17e05a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id  year                                              title  \\\n",
              "1699  2544  2004  A Large Deviation Bound for the Area Under the...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "1699        NaN  2544-a-large-deviation-bound-for-the-area-unde...   \n",
              "\n",
              "              abstract                                         paper_text  \\\n",
              "1699  Abstract Missing  A Large Deviation Bound\\nfor the Area Under th...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "1699  deviation bound area roc curve shivani agarwal...   \n",
              "\n",
              "                                                  Top N  \\\n",
              "1699  [(y, 118), (f, 95), (x, 84), (n, 77), (ranking...   \n",
              "\n",
              "                                           Top_N_TF-IDF  \\\n",
              "1699  [(ranking, 0.479), (ranking function, 0.448), ...   \n",
              "\n",
              "                                          Top_N_KeyBERT  \n",
              "1699  [(ranking, 0.4367), (classification, 0.4331), ...  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Apply KeyBERT to the a sample of the dataset\n",
        "df_[\"Top_N_KeyBERT\"] = df_[\"preproc_text\"].apply(kw_model.extract_keywords)\n",
        "\n",
        "df_.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wYDNwWa5ZiTK"
      },
      "outputs": [],
      "source": [
        "# TODO: compare the same paper example across the 3 methods \n",
        "\n",
        "idx_focus = 121 \n",
        "\n",
        "#df_.loc[121, \"Top_N_KeyBERT_1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GQQGMTZgrR"
      },
      "source": [
        "### Question 2.2. Comparison of multilple techniques [written] (4 points)\n",
        "\n",
        "1. Draw a table of the solution, the quality score that you defined and the time taken to find keywords across a sample of 1000 of the original dataset. \n",
        "2. Can you think of tweaks to reduce time to compute? If yes, add an additional column to the above table with your proposed tweaks.\n",
        "3. Based on the above table and  lecture 1, what do you think is the most appropriate solution for keywords extraction? Why? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhS_Y6qgqW4Z"
      },
      "source": [
        "# Part 2. Word Vectors (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kedMB9f-qsnw",
        "outputId": "fb934e5c-9dcf-45d6-96a0-5457546d92ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy as sp\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5S4yJ_ZrHAo"
      },
      "source": [
        "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore two types of word vectors: those derived from co-occurrence matrices, and those derived via GloVe.\n",
        "\n",
        "Note on Terminology: The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As Wikipedia states, \"conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKcD1SUIrP_m"
      },
      "source": [
        "## Count-Based Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uvm6lbSsFD0"
      },
      "source": [
        "Most word vector models start from the following idea:\n",
        "\n",
        "You shall know a word by the company it keeps (Firth, J. R. 1957:11)\n",
        "\n",
        "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, co-occurrence matrices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMxbozcslLA"
      },
      "source": [
        "## Plotting Co-Occurrence Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3OO_oowsrK2"
      },
      "source": [
        "\n",
        "Here, we will be using the Reuters (business and financial news) corpus. If you haven't run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see https://www.nltk.org/book/ch02.html. We provide a read_corpus function below that pulls out only articles from the \"crude\" (i.e. news articles about oil, gas, etc.) category. The function also adds <START> and <END> tokens to each of the documents, and lowercases words. You do not have to perform any other kind of pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0xTQwympsqDq"
      },
      "outputs": [],
      "source": [
        "def read_corpus(category=\"crude\"):\n",
        "    \"\"\" Read files from the specified Reuter's category.\n",
        "        Params:\n",
        "            category (string): category name\n",
        "        Return:\n",
        "            list of lists, with words from each of the processed files\n",
        "    \"\"\"\n",
        "    files = reuters.fileids(category)\n",
        "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQrwL93ns1Qy"
      },
      "source": [
        "Let's have a look what these documents are like…."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eZvFI3Qs0x4",
        "outputId": "e9ded61f-f13d-4dc3-85dc-448af40b867f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['<START>', 'japan', 'to', 'revise', 'long', '-', 'term', 'energy', 'demand', 'downwards', 'the',\n",
            "  'ministry', 'of', 'international', 'trade', 'and', 'industry', '(', 'miti', ')', 'will', 'revise',\n",
            "  'its', 'long', '-', 'term', 'energy', 'supply', '/', 'demand', 'outlook', 'by', 'august', 'to',\n",
            "  'meet', 'a', 'forecast', 'downtrend', 'in', 'japanese', 'energy', 'demand', ',', 'ministry',\n",
            "  'officials', 'said', '.', 'miti', 'is', 'expected', 'to', 'lower', 'the', 'projection', 'for',\n",
            "  'primary', 'energy', 'supplies', 'in', 'the', 'year', '2000', 'to', '550', 'mln', 'kilolitres',\n",
            "  '(', 'kl', ')', 'from', '600', 'mln', ',', 'they', 'said', '.', 'the', 'decision', 'follows',\n",
            "  'the', 'emergence', 'of', 'structural', 'changes', 'in', 'japanese', 'industry', 'following',\n",
            "  'the', 'rise', 'in', 'the', 'value', 'of', 'the', 'yen', 'and', 'a', 'decline', 'in', 'domestic',\n",
            "  'electric', 'power', 'demand', '.', 'miti', 'is', 'planning', 'to', 'work', 'out', 'a', 'revised',\n",
            "  'energy', 'supply', '/', 'demand', 'outlook', 'through', 'deliberations', 'of', 'committee',\n",
            "  'meetings', 'of', 'the', 'agency', 'of', 'natural', 'resources', 'and', 'energy', ',', 'the',\n",
            "  'officials', 'said', '.', 'they', 'said', 'miti', 'will', 'also', 'review', 'the', 'breakdown',\n",
            "  'of', 'energy', 'supply', 'sources', ',', 'including', 'oil', ',', 'nuclear', ',', 'coal', 'and',\n",
            "  'natural', 'gas', '.', 'nuclear', 'energy', 'provided', 'the', 'bulk', 'of', 'japan', \"'\", 's',\n",
            "  'electric', 'power', 'in', 'the', 'fiscal', 'year', 'ended', 'march', '31', ',', 'supplying',\n",
            "  'an', 'estimated', '27', 'pct', 'on', 'a', 'kilowatt', '/', 'hour', 'basis', ',', 'followed',\n",
            "  'by', 'oil', '(', '23', 'pct', ')', 'and', 'liquefied', 'natural', 'gas', '(', '21', 'pct', '),',\n",
            "  'they', 'noted', '.', '<END>'],\n",
            " ['<START>', 'energy', '/', 'u', '.', 's', '.', 'petrochemical', 'industry', 'cheap', 'oil',\n",
            "  'feedstocks', ',', 'the', 'weakened', 'u', '.', 's', '.', 'dollar', 'and', 'a', 'plant',\n",
            "  'utilization', 'rate', 'approaching', '90', 'pct', 'will', 'propel', 'the', 'streamlined', 'u',\n",
            "  '.', 's', '.', 'petrochemical', 'industry', 'to', 'record', 'profits', 'this', 'year', ',',\n",
            "  'with', 'growth', 'expected', 'through', 'at', 'least', '1990', ',', 'major', 'company',\n",
            "  'executives', 'predicted', '.', 'this', 'bullish', 'outlook', 'for', 'chemical', 'manufacturing',\n",
            "  'and', 'an', 'industrywide', 'move', 'to', 'shed', 'unrelated', 'businesses', 'has', 'prompted',\n",
            "  'gaf', 'corp', '&', 'lt', ';', 'gaf', '>,', 'privately', '-', 'held', 'cain', 'chemical', 'inc',\n",
            "  ',', 'and', 'other', 'firms', 'to', 'aggressively', 'seek', 'acquisitions', 'of', 'petrochemical',\n",
            "  'plants', '.', 'oil', 'companies', 'such', 'as', 'ashland', 'oil', 'inc', '&', 'lt', ';', 'ash',\n",
            "  '>,', 'the', 'kentucky', '-', 'based', 'oil', 'refiner', 'and', 'marketer', ',', 'are', 'also',\n",
            "  'shopping', 'for', 'money', '-', 'making', 'petrochemical', 'businesses', 'to', 'buy', '.', '\"',\n",
            "  'i', 'see', 'us', 'poised', 'at', 'the', 'threshold', 'of', 'a', 'golden', 'period', ',\"', 'said',\n",
            "  'paul', 'oreffice', ',', 'chairman', 'of', 'giant', 'dow', 'chemical', 'co', '&', 'lt', ';',\n",
            "  'dow', '>,', 'adding', ',', '\"', 'there', \"'\", 's', 'no', 'major', 'plant', 'capacity', 'being',\n",
            "  'added', 'around', 'the', 'world', 'now', '.', 'the', 'whole', 'game', 'is', 'bringing', 'out',\n",
            "  'new', 'products', 'and', 'improving', 'the', 'old', 'ones', '.\"', 'analysts', 'say', 'the',\n",
            "  'chemical', 'industry', \"'\", 's', 'biggest', 'customers', ',', 'automobile', 'manufacturers',\n",
            "  'and', 'home', 'builders', 'that', 'use', 'a', 'lot', 'of', 'paints', 'and', 'plastics', ',',\n",
            "  'are', 'expected', 'to', 'buy', 'quantities', 'this', 'year', '.', 'u', '.', 's', '.',\n",
            "  'petrochemical', 'plants', 'are', 'currently', 'operating', 'at', 'about', '90', 'pct',\n",
            "  'capacity', ',', 'reflecting', 'tighter', 'supply', 'that', 'could', 'hike', 'product', 'prices',\n",
            "  'by', '30', 'to', '40', 'pct', 'this', 'year', ',', 'said', 'john', 'dosher', ',', 'managing',\n",
            "  'director', 'of', 'pace', 'consultants', 'inc', 'of', 'houston', '.', 'demand', 'for', 'some',\n",
            "  'products', 'such', 'as', 'styrene', 'could', 'push', 'profit', 'margins', 'up', 'by', 'as',\n",
            "  'much', 'as', '300', 'pct', ',', 'he', 'said', '.', 'oreffice', ',', 'speaking', 'at', 'a',\n",
            "  'meeting', 'of', 'chemical', 'engineers', 'in', 'houston', ',', 'said', 'dow', 'would', 'easily',\n",
            "  'top', 'the', '741', 'mln', 'dlrs', 'it', 'earned', 'last', 'year', 'and', 'predicted', 'it',\n",
            "  'would', 'have', 'the', 'best', 'year', 'in', 'its', 'history', '.', 'in', '1985', ',', 'when',\n",
            "  'oil', 'prices', 'were', 'still', 'above', '25', 'dlrs', 'a', 'barrel', 'and', 'chemical',\n",
            "  'exports', 'were', 'adversely', 'affected', 'by', 'the', 'strong', 'u', '.', 's', '.', 'dollar',\n",
            "  ',', 'dow', 'had', 'profits', 'of', '58', 'mln', 'dlrs', '.', '\"', 'i', 'believe', 'the',\n",
            "  'entire', 'chemical', 'industry', 'is', 'headed', 'for', 'a', 'record', 'year', 'or', 'close',\n",
            "  'to', 'it', ',\"', 'oreffice', 'said', '.', 'gaf', 'chairman', 'samuel', 'heyman', 'estimated',\n",
            "  'that', 'the', 'u', '.', 's', '.', 'chemical', 'industry', 'would', 'report', 'a', '20', 'pct',\n",
            "  'gain', 'in', 'profits', 'during', '1987', '.', 'last', 'year', ',', 'the', 'domestic',\n",
            "  'industry', 'earned', 'a', 'total', 'of', '13', 'billion', 'dlrs', ',', 'a', '54', 'pct', 'leap',\n",
            "  'from', '1985', '.', 'the', 'turn', 'in', 'the', 'fortunes', 'of', 'the', 'once', '-', 'sickly',\n",
            "  'chemical', 'industry', 'has', 'been', 'brought', 'about', 'by', 'a', 'combination', 'of', 'luck',\n",
            "  'and', 'planning', ',', 'said', 'pace', \"'\", 's', 'john', 'dosher', '.', 'dosher', 'said', 'last',\n",
            "  'year', \"'\", 's', 'fall', 'in', 'oil', 'prices', 'made', 'feedstocks', 'dramatically', 'cheaper',\n",
            "  'and', 'at', 'the', 'same', 'time', 'the', 'american', 'dollar', 'was', 'weakening', 'against',\n",
            "  'foreign', 'currencies', '.', 'that', 'helped', 'boost', 'u', '.', 's', '.', 'chemical',\n",
            "  'exports', '.', 'also', 'helping', 'to', 'bring', 'supply', 'and', 'demand', 'into', 'balance',\n",
            "  'has', 'been', 'the', 'gradual', 'market', 'absorption', 'of', 'the', 'extra', 'chemical',\n",
            "  'manufacturing', 'capacity', 'created', 'by', 'middle', 'eastern', 'oil', 'producers', 'in',\n",
            "  'the', 'early', '1980s', '.', 'finally', ',', 'virtually', 'all', 'major', 'u', '.', 's', '.',\n",
            "  'chemical', 'manufacturers', 'have', 'embarked', 'on', 'an', 'extensive', 'corporate',\n",
            "  'restructuring', 'program', 'to', 'mothball', 'inefficient', 'plants', ',', 'trim', 'the',\n",
            "  'payroll', 'and', 'eliminate', 'unrelated', 'businesses', '.', 'the', 'restructuring', 'touched',\n",
            "  'off', 'a', 'flurry', 'of', 'friendly', 'and', 'hostile', 'takeover', 'attempts', '.', 'gaf', ',',\n",
            "  'which', 'made', 'an', 'unsuccessful', 'attempt', 'in', '1985', 'to', 'acquire', 'union',\n",
            "  'carbide', 'corp', '&', 'lt', ';', 'uk', '>,', 'recently', 'offered', 'three', 'billion', 'dlrs',\n",
            "  'for', 'borg', 'warner', 'corp', '&', 'lt', ';', 'bor', '>,', 'a', 'chicago', 'manufacturer',\n",
            "  'of', 'plastics', 'and', 'chemicals', '.', 'another', 'industry', 'powerhouse', ',', 'w', '.',\n",
            "  'r', '.', 'grace', '&', 'lt', ';', 'gra', '>', 'has', 'divested', 'its', 'retailing', ',',\n",
            "  'restaurant', 'and', 'fertilizer', 'businesses', 'to', 'raise', 'cash', 'for', 'chemical',\n",
            "  'acquisitions', '.', 'but', 'some', 'experts', 'worry', 'that', 'the', 'chemical', 'industry',\n",
            "  'may', 'be', 'headed', 'for', 'trouble', 'if', 'companies', 'continue', 'turning', 'their',\n",
            "  'back', 'on', 'the', 'manufacturing', 'of', 'staple', 'petrochemical', 'commodities', ',', 'such',\n",
            "  'as', 'ethylene', ',', 'in', 'favor', 'of', 'more', 'profitable', 'specialty', 'chemicals',\n",
            "  'that', 'are', 'custom', '-', 'designed', 'for', 'a', 'small', 'group', 'of', 'buyers', '.', '\"',\n",
            "  'companies', 'like', 'dupont', '&', 'lt', ';', 'dd', '>', 'and', 'monsanto', 'co', '&', 'lt', ';',\n",
            "  'mtc', '>', 'spent', 'the', 'past', 'two', 'or', 'three', 'years', 'trying', 'to', 'get', 'out',\n",
            "  'of', 'the', 'commodity', 'chemical', 'business', 'in', 'reaction', 'to', 'how', 'badly', 'the',\n",
            "  'market', 'had', 'deteriorated', ',\"', 'dosher', 'said', '.', '\"', 'but', 'i', 'think', 'they',\n",
            "  'will', 'eventually', 'kill', 'the', 'margins', 'on', 'the', 'profitable', 'chemicals', 'in',\n",
            "  'the', 'niche', 'market', '.\"', 'some', 'top', 'chemical', 'executives', 'share', 'the',\n",
            "  'concern', '.', '\"', 'the', 'challenge', 'for', 'our', 'industry', 'is', 'to', 'keep', 'from',\n",
            "  'getting', 'carried', 'away', 'and', 'repeating', 'past', 'mistakes', ',\"', 'gaf', \"'\", 's',\n",
            "  'heyman', 'cautioned', '.', '\"', 'the', 'shift', 'from', 'commodity', 'chemicals', 'may', 'be',\n",
            "  'ill', '-', 'advised', '.', 'specialty', 'businesses', 'do', 'not', 'stay', 'special', 'long',\n",
            "  '.\"', 'houston', '-', 'based', 'cain', 'chemical', ',', 'created', 'this', 'month', 'by', 'the',\n",
            "  'sterling', 'investment', 'banking', 'group', ',', 'believes', 'it', 'can', 'generate', '700',\n",
            "  'mln', 'dlrs', 'in', 'annual', 'sales', 'by', 'bucking', 'the', 'industry', 'trend', '.',\n",
            "  'chairman', 'gordon', 'cain', ',', 'who', 'previously', 'led', 'a', 'leveraged', 'buyout', 'of',\n",
            "  'dupont', \"'\", 's', 'conoco', 'inc', \"'\", 's', 'chemical', 'business', ',', 'has', 'spent', '1',\n",
            "  '.', '1', 'billion', 'dlrs', 'since', 'january', 'to', 'buy', 'seven', 'petrochemical', 'plants',\n",
            "  'along', 'the', 'texas', 'gulf', 'coast', '.', 'the', 'plants', 'produce', 'only', 'basic',\n",
            "  'commodity', 'petrochemicals', 'that', 'are', 'the', 'building', 'blocks', 'of', 'specialty',\n",
            "  'products', '.', '\"', 'this', 'kind', 'of', 'commodity', 'chemical', 'business', 'will', 'never',\n",
            "  'be', 'a', 'glamorous', ',', 'high', '-', 'margin', 'business', ',\"', 'cain', 'said', ',',\n",
            "  'adding', 'that', 'demand', 'is', 'expected', 'to', 'grow', 'by', 'about', 'three', 'pct',\n",
            "  'annually', '.', 'garo', 'armen', ',', 'an', 'analyst', 'with', 'dean', 'witter', 'reynolds', ',',\n",
            "  'said', 'chemical', 'makers', 'have', 'also', 'benefitted', 'by', 'increasing', 'demand', 'for',\n",
            "  'plastics', 'as', 'prices', 'become', 'more', 'competitive', 'with', 'aluminum', ',', 'wood',\n",
            "  'and', 'steel', 'products', '.', 'armen', 'estimated', 'the', 'upturn', 'in', 'the', 'chemical',\n",
            "  'business', 'could', 'last', 'as', 'long', 'as', 'four', 'or', 'five', 'years', ',', 'provided',\n",
            "  'the', 'u', '.', 's', '.', 'economy', 'continues', 'its', 'modest', 'rate', 'of', 'growth', '.',\n",
            "  '<END>'],\n",
            " ['<START>', 'turkey', 'calls', 'for', 'dialogue', 'to', 'solve', 'dispute', 'turkey', 'said',\n",
            "  'today', 'its', 'disputes', 'with', 'greece', ',', 'including', 'rights', 'on', 'the',\n",
            "  'continental', 'shelf', 'in', 'the', 'aegean', 'sea', ',', 'should', 'be', 'solved', 'through',\n",
            "  'negotiations', '.', 'a', 'foreign', 'ministry', 'statement', 'said', 'the', 'latest', 'crisis',\n",
            "  'between', 'the', 'two', 'nato', 'members', 'stemmed', 'from', 'the', 'continental', 'shelf',\n",
            "  'dispute', 'and', 'an', 'agreement', 'on', 'this', 'issue', 'would', 'effect', 'the', 'security',\n",
            "  ',', 'economy', 'and', 'other', 'rights', 'of', 'both', 'countries', '.', '\"', 'as', 'the',\n",
            "  'issue', 'is', 'basicly', 'political', ',', 'a', 'solution', 'can', 'only', 'be', 'found', 'by',\n",
            "  'bilateral', 'negotiations', ',\"', 'the', 'statement', 'said', '.', 'greece', 'has', 'repeatedly',\n",
            "  'said', 'the', 'issue', 'was', 'legal', 'and', 'could', 'be', 'solved', 'at', 'the',\n",
            "  'international', 'court', 'of', 'justice', '.', 'the', 'two', 'countries', 'approached', 'armed',\n",
            "  'confrontation', 'last', 'month', 'after', 'greece', 'announced', 'it', 'planned', 'oil',\n",
            "  'exploration', 'work', 'in', 'the', 'aegean', 'and', 'turkey', 'said', 'it', 'would', 'also',\n",
            "  'search', 'for', 'oil', '.', 'a', 'face', '-', 'off', 'was', 'averted', 'when', 'turkey',\n",
            "  'confined', 'its', 'research', 'to', 'territorrial', 'waters', '.', '\"', 'the', 'latest',\n",
            "  'crises', 'created', 'an', 'historic', 'opportunity', 'to', 'solve', 'the', 'disputes', 'between',\n",
            "  'the', 'two', 'countries', ',\"', 'the', 'foreign', 'ministry', 'statement', 'said', '.', 'turkey',\n",
            "  \"'\", 's', 'ambassador', 'in', 'athens', ',', 'nazmi', 'akiman', ',', 'was', 'due', 'to', 'meet',\n",
            "  'prime', 'minister', 'andreas', 'papandreou', 'today', 'for', 'the', 'greek', 'reply', 'to', 'a',\n",
            "  'message', 'sent', 'last', 'week', 'by', 'turkish', 'prime', 'minister', 'turgut', 'ozal', '.',\n",
            "  'the', 'contents', 'of', 'the', 'message', 'were', 'not', 'disclosed', '.', '<END>']]\n"
          ]
        }
      ],
      "source": [
        "reuters_corpus = read_corpus()\n",
        "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNKy6j3as7xJ"
      },
      "source": [
        "### Question 2.1: Implement distinct_words [code] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIgkQ47otdqZ"
      },
      "source": [
        "Write a method to work out the distinct words (word types) that occur in the corpus. You can do this with for loops, but it's more efficient to do it with Python list comprehensions. In particular, this may be useful to flatten a list of lists. If you're not familiar with Python list comprehensions in general, here's more information.\n",
        "\n",
        "Your returned corpus_words should be sorted. You can use python's sorted function for this.\n",
        "\n",
        "You may find it useful to use Python sets to remove duplicate words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VTIH5vFetgjD"
      },
      "outputs": [],
      "source": [
        "def distinct_words(corpus):\n",
        "    \"\"\" Determine a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents - eg [[\"hey\", \"I\", \"am\", \"toto\"], [\"hey\", \"I\", \"am\", \"tata\"]]\n",
        "        Return:\n",
        "            corpus_words (list of strings): sorted list of distinct words across the corpus\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"\n",
        "    corpus_words = []\n",
        "    num_corpus_words = -1\n",
        "\n",
        "    distinct_words = sorted(list({word for doc in corpus for word in doc}))\n",
        "    num_distinct_words = len(distinct_words)\n",
        "    return distinct_words, num_distinct_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZX4dH8stmYN",
        "outputId": "ebd1eb7d-6fc2-4d62-cf00-3ae49753abb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
        "\n",
        "# Correct answers\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "ans_num_corpus_words = len(ans_test_corpus_words)\n",
        "\n",
        "# Test correct number of words\n",
        "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
        "\n",
        "# Test correct words\n",
        "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86fD2hYr3fw8"
      },
      "source": [
        "### Question 2.2: Implement compute_co_occurrence_matrix [code] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE4MLCIa3lKw"
      },
      "source": [
        "Write a method that constructs a co-occurrence matrix for a certain window-size  n  (with a default of 4), considering words  n  before and  n  after the word in the center of the window. Here, we start to use numpy (np) to represent vectors, matrices, and tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zz5vrGb43lbA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from collections import Counter \n",
        "\n",
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "    \n",
        "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "              number of co-occurring words.\n",
        "              \n",
        "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
        "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "    \n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
        "                Co-occurence matrix of word counts. \n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    M = None\n",
        "    word2ind = {}\n",
        "\n",
        "    M = np.zeros((num_words, num_words))\n",
        "    word2ind = {word: i for i, word in enumerate(words)}\n",
        "    for doc in corpus:\n",
        "        for i, word in enumerate(doc):\n",
        "            for j in range(max(0, i - window_size), min(len(doc), i + window_size + 1)):\n",
        "                if i != j:\n",
        "                    M[word2ind[word], word2ind[doc[j]]] += 1\n",
        "\n",
        "    return M, word2ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guUdCsM2BUuC",
        "outputId": "945c9bb1-a262-4881-abb4-ea4cf96d1dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and get student's co-occurrence matrix\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "\n",
        "# Correct M and word2ind\n",
        "M_test_ans = np.array( \n",
        "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
        "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
        "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
        "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
        "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
        ")\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "word2ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
        "\n",
        "# Test correct word2ind\n",
        "assert (word2ind_ans == word2ind_test), \"Your word2ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2ind_ans, word2ind_test)\n",
        "\n",
        "# Test correct M shape\n",
        "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
        "\n",
        "# Test correct M values\n",
        "for w1 in word2ind_ans.keys():\n",
        "    idx1 = word2ind_ans[w1]\n",
        "    for w2 in word2ind_ans.keys():\n",
        "        idx2 = word2ind_ans[w2]\n",
        "        student = M_test[idx1, idx2]\n",
        "        correct = M_test_ans[idx1, idx2]\n",
        "        if student != correct:\n",
        "            print(\"Correct M:\")\n",
        "            print(M_test_ans)\n",
        "            print(\"Your M: \")\n",
        "            print(M_test)\n",
        "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCC24T0WPyI2"
      },
      "source": [
        "### Question 2.3: Implement reduce_to_k_dim [code] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ9XXG-WP2dZ"
      },
      "source": [
        "Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings.\n",
        "\n",
        "Note: All of numpy, scipy, and scikit-learn (sklearn) provide some implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use sklearn.decomposition.TruncatedSVD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5jfqvOUOP8R6"
      },
      "outputs": [],
      "source": [
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "    \n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"    \n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "    \n",
        "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
        "    M_reduced = svd.fit_transform(M)\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rGeaWNuRAnJ",
        "outputId": "42a1bdf0-af99-49f8-eb19-119a28f86e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Truncated SVD over 10 words...\n",
            "Done.\n",
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness \n",
        "# In fact we only check that your M_reduced has the right dimensions.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and run student code\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
        "\n",
        "# Test proper dimensions\n",
        "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
        "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iTgMaquRQKB"
      },
      "source": [
        "### Question 2.4: Implement plot_embeddings [code] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H629WACPRTg2"
      },
      "source": [
        "Here you will write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (plt).\n",
        "\n",
        "For this example, you may find it useful to adapt this code. In the future, a good way to make a plot is to look at the Matplotlib gallery, find a plot that looks somewhat like what you want, and adapt the code they give."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lMfaxKfERT1P"
      },
      "outputs": [],
      "source": [
        "def plot_embeddings(M_reduced, word2ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        NOTE: do not plot all the words listed in M_reduced / word2ind.\n",
        "        Include a label next to each point.\n",
        "        \n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus , 2)): matrix of 2-dimensioal word embeddings\n",
        "            word2ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a scatter plot and plot embeddings of each word\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "    for word in words:\n",
        "        idx = word2ind[word]\n",
        "        x, y = M_reduced[idx, :]\n",
        "        ax.scatter(y, x, marker='o', color='purple', s=400)\n",
        "        ax.annotate(word, (y, x), fontsize=12)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "bJ5sOXmXRYOa",
        "outputId": "5f07c06e-3c33-497b-dee3-2d93ca8eccee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Outputted Plot:\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# The plot produced should look like the \"test solution plot\" depicted below. \n",
        "# ---------------------\n",
        "\n",
        "print (\"-\" * 80)\n",
        "print (\"Outputted Plot:\")\n",
        "\n",
        "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
        "word2ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
        "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
        "plot_embeddings(M_reduced_plot_test, word2ind_plot_test, words)\n",
        "\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRb0HnWqVCDL"
      },
      "source": [
        "### Question 2.5: Co-Occurrence Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkfAWdLFVFx-"
      },
      "source": [
        "Now we will put together all the parts you have written! We will compute the co-occurrence matrix with fixed window of 4 (the default window size), over the Reuters \"crude\" (oil) corpus. Then we will use TruncatedSVD to compute 2-dimensional embeddings of each word. TruncatedSVD returns U*S, so we need to normalize the returned vectors, so that all the vectors will appear around the unit circle (therefore closeness is directional closeness). Note: The line of code below that does the normalizing uses the NumPy concept of broadcasting. If you don't know about broadcasting, check out Computation on Arrays: Broadcasting by Jake VanderPlas.\n",
        "\n",
        "Run the below cell to produce the plot. It'll probably take a few seconds to run. What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? Note: \"bpd\" stands for \"barrels per day\" and is a commonly used abbreviation in crude oil topic articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "R3d0UK2iVFDM",
        "outputId": "03df5118-c50e-4878-b843-cb3cbe7cd79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Truncated SVD over 8185 words...\n",
            "Done.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Run This Cell to Produce Your Plot\n",
        "# ------------------------------\n",
        "reuters_corpus = read_corpus()\n",
        "M_co_occurrence, word2ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
        "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
        "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
        "\n",
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "\n",
        "plot_embeddings(M_normalized, word2ind_co_occurrence, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQg7l284V0oa"
      },
      "source": [
        "# Part 3. Prediction-based word vectors (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSNq-K6UzzDP"
      },
      "source": [
        "As discussed in class, more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). If you're feeling adventurous, challenge yourself and try reading GloVe's original paper.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory. Note: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kqq7A2IWz011"
      },
      "outputs": [],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
        "    return wv_from_bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYbJ59Jiz7OA",
        "outputId": "d5ce98e1-03ea-4d98-b14a-be7cff78cbcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This will take a couple minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edzctdyh0rDm"
      },
      "source": [
        "#### Note: If you are receiving a \"reset by peer\" error, rerun the cell to restart the download."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbGSRVPxjaT_"
      },
      "source": [
        "## Reducing dimensionality of Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXr1zGXSjddn"
      },
      "source": [
        "Let's directly compare the GloVe embeddings to those of the co-occurrence matrix. In order to avoid running out of memory, we will work with a sample of 10000 GloVe vectors instead. Run the following cells to:\n",
        "\n",
        "Put 10000 Glove vectors into a matrix M\n",
        "Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 200-dimensional to 2-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "x_xj1ApzjfOr"
      },
      "outputs": [],
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']):\n",
        "    \"\"\" Put the GloVe vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 200) containing the vectors\n",
        "            word2ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    words = list(wv_from_bin.vocab.keys())\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.seed(224)\n",
        "    random.shuffle(words)\n",
        "    words = words[:10000]\n",
        "    print(\"Putting %i words into word2ind and matrix M...\" % len(words))\n",
        "    word2ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    for w in required_words:\n",
        "        if w in words:\n",
        "            continue\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHVTZLiBjhN5",
        "outputId": "9155e7b3-9d95-47d1-eac7-37c0f1869cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shuffling words ...\n",
            "Putting 10000 words into word2ind and matrix M...\n",
            "Done.\n",
            "Running Truncated SVD over 10010 words...\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions\n",
        "# Note: This should be quick to run\n",
        "# -----------------------------------------------------------------\n",
        "M, word2ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
        "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdoZKWxijmHk"
      },
      "source": [
        "### Question 3.1: GloVe Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyDIugqjjo3R"
      },
      "source": [
        "Run the cell below to plot the 2D GloVe embeddings for ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq'].\n",
        "\n",
        "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you think should have? How is the plot different from the one generated earlier from the co-occurrence matrix? What is a possible cause for the difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "KK438bCgjm98",
        "outputId": "dc22cadd-b420-42ec-e632-72e015e42433"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "plot_embeddings(M_reduced_normalized, word2ind, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39cCl0DQjuJN"
      },
      "source": [
        "In a 2D embedding space, words that cluster together are words that have similar contexts, i.e. words that have a common meaning or are related.\n",
        "\n",
        "The words \"barrels\" and \"petroleum\" could have been grouped together as they are related but are not. This may be due to the fact that the two words are not frequent enough in the corpus.\n",
        "\n",
        "How is the plot different from the one generated earlier from the co-occurrence matrix? What is a possible cause for the difference?\n",
        "\n",
        "The plot is different from the one generated earlier from the co-occurrence matrix because the words are not grouped together in the same way, now there are word clusters. This is because the co-occurrence matrix is a symmetric matrix, whereas the word2vec model is not and the representation of words are not the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toHZ2o-Ljwwm"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBZbtLYzj1R_"
      },
      "source": [
        "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
        "\n",
        "We can think of n-dimensional vectors as points in n-dimensional space. If we take this perspective L1 and L2 Distances help quantify the amount of space \"we must travel\" to get between these two points. Another approach is to examine the angle between two vectors. From trigonometry we know that:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLE4DOTNj69N"
      },
      "source": [
        "### Question 3.2: Words with Multiple Meanings (1.5 points) [code + written]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0LT0bH4kolN"
      },
      "source": [
        "Polysemes and homonyms are words that have more than one meaning (see this wiki page to learn more about the difference between polysemes and homonyms ). Find a word with at least two different meanings such that the top-10 most similar words (according to cosine similarity) contain related words from both meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain one of the meanings of the words)?\n",
        "\n",
        "Note: You should use the wv_from_bin.most_similar(word) function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the GenSim documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JgoE4V3rj76o",
        "outputId": "2d724c72-0c01-4634-c2fd-bca1ede1540b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'barrels'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def find_closest_word(word, M_reduced, word2ind):\n",
        "    \"\"\" Find the closest word in the reduced embedding space to the given word.\n",
        "        Params:\n",
        "            word (string): query word\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus , 2)): matrix of 2-dimensioal word embeddings\n",
        "            word2ind (dict): dictionary that maps word to indices for matrix M\n",
        "        Return:\n",
        "            closest_word (string): word in the corpus that is closest to the given word\n",
        "    \"\"\"\n",
        "    # Get the word's embedding. Use the word2ind dictionary to get the index.\n",
        "    idx = word2ind[word]\n",
        "    word_embedding = M_reduced[idx, :]\n",
        "    \n",
        "    # Compute distances between this word and all others.\n",
        "    # You can do this using broadcasting.\n",
        "    distances = np.linalg.norm(M_reduced - word_embedding, axis=1)\n",
        "    \n",
        "    # Find the index of the closest word embedding.\n",
        "    # Don't include the distance to the word itself, set it to infinity.\n",
        "    closest_idx = np.argmin(distances)\n",
        "    \n",
        "    # Use the index to find the closest word.\n",
        "    closest_word = list(word2ind.keys())[list(word2ind.values()).index(closest_idx)]\n",
        "    \n",
        "    return closest_word\n",
        "\n",
        "find_closest_word('barrels', M_reduced_normalized, word2ind)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z90e-p_jktYD"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBt1A0Y8kyqx"
      },
      "source": [
        "### Question 3.3: Synonyms & Antonyms (2 points) [code + written]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2gWr_Cvk3Tu"
      },
      "source": [
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words  (w1,w2,w3)  where  w1  and  w2  are synonyms and  w1  and  w3  are antonyms, but Cosine Distance  (w1,w3)<  Cosine Distance  (w1,w2) .\n",
        "\n",
        "As an example,  w1 =\"happy\" is closer to  w3 =\"sad\" than to  w2 =\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the wv_from_bin.distance(w1, w2) function here in order to compute the cosine distance between two words. Please see the GenSim documentation for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRwHp4noktJF",
        "outputId": "166a7e33-2a83-4208-d850-f11aa45a3f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def condition_between_synonyms_antonyms(word1, word2, word3):\n",
        "  syn_dist =  wv_from_bin.distance(word1, word2)\n",
        "  ant_dist =  wv_from_bin.distance(word1, word3)\n",
        "\n",
        "  return ant_dist < syn_dist\n",
        "\n",
        "# Test the exemple\n",
        "assert condition_between_synonyms_antonyms(\"happy\", \"cheerful\", \"sad\")\n",
        "\n",
        "# Test our proposal \n",
        "assert condition_between_synonyms_antonyms(\"hot\", \"warm\", \"cold\")\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AD2asvrk7Y7"
      },
      "source": [
        "One possible explanation is that the word vectors of synonyms and antonyms are not perfectly orthogonal to each other in high-dimensional space. In other words, the vectors of synonyms and antonyms may not be perfectly correlated and may overlap. For example, in the case of \"hot\" and \"cold\", the word vectors of these two words may not be perfectly orthogonal, even if they are antonyms. This may be because the two words are often used to describe temperature, and the vectors of these words may have common elements that capture this meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0uNtlXZlAdy"
      },
      "source": [
        "### Question 3.4: Analogies with Word Vectors [written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqKCiSDhlEBf"
      },
      "source": [
        "Word vectors have been shown to sometimes exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : king :: woman : x\" (read: man is to king as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the most_similar function from the GenSim documentation. The function finds words that are most similar to the words in the positive list and most dissimilar from the words in the negative list (while omitting the input words, which are often the most similar; see this paper). The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHlsY4kolA6f",
        "outputId": "8dd8549e-f3d4-4594-c138-bab880eeae7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('queen', 0.6978678703308105),\n",
            " ('princess', 0.6081745028495789),\n",
            " ('monarch', 0.5889754891395569),\n",
            " ('throne', 0.5775108933448792),\n",
            " ('prince', 0.5750998854637146),\n",
            " ('elizabeth', 0.546359658241272),\n",
            " ('daughter', 0.5399125814437866),\n",
            " ('kingdom', 0.5318052768707275),\n",
            " ('mother', 0.5168544054031372),\n",
            " ('crown', 0.5164472460746765)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to answer the analogy -- man : king :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv1gqPl5nQ4H"
      },
      "source": [
        "Let  m ,  k ,  w , and  x  denote the word vectors for man, king, woman, and the answer, respectively. Using only vectors  m ,  k ,  w , and the vector arithmetic operators  +  and  −  in your answer, what is the expression in which we are maximizing cosine similarity with  x ?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would man and woman lie in the coordinate plane relative to king and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zvj-YaOnTAY"
      },
      "source": [
        "The expression in which we maximise the similarity of the cosine with x is w - m + k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyzhOOfnuql"
      },
      "source": [
        "### Question 3.5: Finding Analogies [code + written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5xFEsKVnzT0"
      },
      "source": [
        "Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "Note: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkrWfGGznRMH",
        "outputId": "4ae04d64-0c8d-4cf5-cc24-2b3e2f2643a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('vegetables', 0.7133320569992065),\n",
            " ('fruits', 0.6467835903167725),\n",
            " ('beans', 0.6283371448516846),\n",
            " ('cooked', 0.6031776666641235),\n",
            " ('vegetable', 0.5949727892875671),\n",
            " ('berries', 0.5880725383758545),\n",
            " ('meat', 0.5720727443695068),\n",
            " ('mashed', 0.5708385705947876),\n",
            " ('bananas', 0.5619319081306458),\n",
            " ('lentils', 0.5608645081520081)]\n"
          ]
        }
      ],
      "source": [
        "def find_analogies(x, y, a):\n",
        "  return pprint.pprint(wv_from_bin.most_similar(positive=[a, y], negative=[x]))\n",
        "\n",
        "find_analogies(\"apple\", \"fruit\", \"potatoes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NygUqza7n8mn"
      },
      "source": [
        "One example of an analogy that holds according to these vectors could be:\n",
        "\n",
        "\"Apple: Fruit :: Potatoes: Vegetables\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QMbSmc52n-Ni"
      },
      "source": [
        "### Question 3.6: Incorrect Analogy [code + written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKhHs5uooAaD"
      },
      "source": [
        "Find an example of analogy that does not hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayAfn_MPnzrF",
        "outputId": "a6eb3215-6f49-4655-ba9d-b2d07b15978c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('rights', 0.5145905613899231),\n",
            " ('beings', 0.473871111869812),\n",
            " ('blood', 0.47122642397880554),\n",
            " ('environment', 0.459567129611969),\n",
            " ('humanity', 0.45369186997413635),\n",
            " ('earth', 0.4464641511440277),\n",
            " ('humanitarian', 0.4438990354537964),\n",
            " ('humans', 0.4417664408683777),\n",
            " ('basic', 0.44106465578079224),\n",
            " ('life', 0.44028201699256897)]\n"
          ]
        }
      ],
      "source": [
        "find_analogies(\"plant\", \"water\", \"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqq_EaXoDYl"
      },
      "source": [
        "One example of analogy that does not hold according to these vectors couls be :\n",
        "\n",
        "\"Plant : Water :: Human : Food\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBlWGAmoGCy"
      },
      "source": [
        "### Question 3.7: Guided Analysis of Bias in Word Vectors [written] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_rVPQteoINq"
      },
      "source": [
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"worker\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"worker\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDlvsts2oBsp",
        "outputId": "ca7db874-ce3b-462a-dbc2-29e00da59ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('employee', 0.6375863552093506),\n",
            " ('workers', 0.6068919897079468),\n",
            " ('nurse', 0.5837947726249695),\n",
            " ('pregnant', 0.5363885164260864),\n",
            " ('mother', 0.5321309566497803),\n",
            " ('employer', 0.5127025842666626),\n",
            " ('teacher', 0.5099576711654663),\n",
            " ('child', 0.5096741914749146),\n",
            " ('homemaker', 0.5019454956054688),\n",
            " ('nurses', 0.4970572590827942)]\n",
            "\n",
            "[('workers', 0.6113258004188538),\n",
            " ('employee', 0.5983108282089233),\n",
            " ('working', 0.5615328550338745),\n",
            " ('laborer', 0.5442320108413696),\n",
            " ('unemployed', 0.5368517637252808),\n",
            " ('job', 0.5278826951980591),\n",
            " ('work', 0.5223963260650635),\n",
            " ('mechanic', 0.5088937282562256),\n",
            " ('worked', 0.505452036857605),\n",
            " ('factory', 0.4940453767776489)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
        "# most dissimilar from.\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'worker'], negative=['man']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'worker'], negative=['woman']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BtOxfydoLZb"
      },
      "source": [
        "The list of words associated with women is very much related to parenting and the home. While the men's list is related to factory work. This highlights the gender bias because today men and women do the same jobs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMsfYJyxoNWT"
      },
      "source": [
        "###  Question 3.8: Independent Analysis of Bias in Word Vectors [code + written] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h42ajQDcodeV"
      },
      "source": [
        "Use the most_similar function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "N9d5cbx3oJsf"
      },
      "outputs": [],
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JDBxYCeEolk5"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNaetQ4donRi"
      },
      "source": [
        "### Question 3.9: Thinking About Bias [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mktU3tdqtzp"
      },
      "source": [
        "Give one explanation of how bias gets into the word vectors. What is an experiment that you could do to test for or to measure this source of bias?\n",
        "\n",
        "One explanation of how bias gets into the word vectors is that the word vectors are trained on a corpus of text that is biased. For example, if the corpus is a collection of news articles, the word vectors will be biased towards words that are used in news articles. This is because the word vectors are trained on the co-occurrence matrix of the corpus, which is a matrix that counts the number of times each word occurs in the context of each other word. If the corpus is biased, the word vectors will also be biased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrXQ2d7OsmLl"
      },
      "source": [
        "# Part 4. Prediction-based sentence vectors (13 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOP9j0mV2rvU"
      },
      "source": [
        "Sentence embeddings are a more powerful representation than word embeddings. They allow you to have out-of-the-box sentence representation of sequences of tokens which is closer to what you would have in reality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ3pVRQ8wE4P"
      },
      "source": [
        "### Question 4.1: How would you represent a sentence with Glove? What are the limits of your proposed implementation? [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1w30iYj0YJn"
      },
      "source": [
        "We concatenate the vectors (obtained with GloVe) of the words in the sentence into a single long vector.\n",
        "\n",
        "A limitation of concatenating all GloVe vectors to represent a sentence is that the resulting vector will be very long, potentially hundreds or thousands of dimensions depending on the number of words in the sentence. This can make it difficult to work with the vector, as it can require a lot of computing resources to process and store. .\n",
        "\n",
        "Finally, vector concatenation does not take into account the order and relationships between words in the sentence. This can be a problem for tasks that require a deeper understanding of meaning and sentence structure, such as translation or text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2n3vp8g3AEe"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "s9F_tbMr3CZT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nBTJnO606Tpc"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "NnidR9Gg2697"
      },
      "outputs": [],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load SentenceBERT Vectors\n",
        "        Return:\n",
        "            embedder: sentence embedder \n",
        "    \"\"\"\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    return embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "1qzL9oBS4Zoc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "embedder = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKkFzRHe6bEm"
      },
      "source": [
        "Inspired by the above, choose the appropriate way to plot the below clusters. Do they make sense to you? What would you improve to get a meaningful plot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-o3T8Wz-Feb"
      },
      "source": [
        "### Question 4.2. Evaluate clustering quality of SentenceBERT. What makes it good at clustering sentences? Which method of the two below would you go for? [written] (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqHOmWtqyNo3",
        "outputId": "df5f60b6-584d-4db3-eff9-28a98b302d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster  1\n",
            "['The girl is carrying a baby.', 'The baby is carried by the woman']\n",
            "\n",
            "Cluster  2\n",
            "['A man is riding a horse.', 'A man is riding a white horse on an enclosed ground.']\n",
            "\n",
            "Cluster  3\n",
            "['A cheetah is running behind its prey.', 'A cheetah chases prey on across a field.']\n",
            "\n",
            "Cluster  4\n",
            "['A man is eating food.', 'A man is eating a piece of bread.', 'A man is eating pasta.']\n",
            "\n",
            "Cluster  5\n",
            "['A monkey is playing drums.', 'Someone in a gorilla costume is playing a set of drums.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'A man is eating pasta.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'The baby is carried by the woman',\n",
        "          'A man is riding a horse.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'Someone in a gorilla costume is playing a set of drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'A cheetah chases prey on across a field.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Perform kmean clustering\n",
        "num_clusters = 5\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4wF5uTy50Nt",
        "outputId": "88013cef-9eb0-4126-da8b-920d8eddcbaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster  1\n",
            "['A man is eating food.', 'A man is eating a piece of bread.', 'A man is eating pasta.']\n",
            "\n",
            "Cluster  5\n",
            "['The girl is carrying a baby.', 'The baby is carried by the woman']\n",
            "\n",
            "Cluster  2\n",
            "['A man is riding a horse.', 'A man is riding a white horse on an enclosed ground.']\n",
            "\n",
            "Cluster  3\n",
            "['A monkey is playing drums.', 'Someone in a gorilla costume is playing a set of drums.']\n",
            "\n",
            "Cluster  4\n",
            "['A cheetah is running behind its prey.', 'A cheetah chases prey on across a field.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'A man is eating pasta.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'The baby is carried by the woman',\n",
        "          'A man is riding a horse.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'Someone in a gorilla costume is playing a set of drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'A cheetah chases prey on across a field.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Perform kmean clustering\n",
        "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44mDLKmVyOUu"
      },
      "source": [
        "### Question 4.3: SentenceBERT Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ykELvWt7ZX"
      },
      "source": [
        "Plot the above corpus with your favorite method in a 2-dimensional space. Comment on the output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqSyMyy-bm8"
      },
      "source": [
        "### Question 4.4: Independent Analysis of Bias in Word Vectors [code + written] (4 points) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZynvZnq-nqk"
      },
      "source": [
        "Select a corpus of interest, or examples of interest and shed light on one source of bias from SentenceBERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "DQyOEVUWvEmI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "C2IElpuJ76o0",
        "BHC9ShM-IX7E",
        "KK7YbNCyJaqk",
        "Wao4NivXGPIM",
        "A1wbWizrJ5CE",
        "x9hzsg0CUQZ_",
        "mhS_Y6qgqW4Z",
        "wRb0HnWqVCDL",
        "hQg7l284V0oa",
        "qbGSRVPxjaT_",
        "cdoZKWxijmHk",
        "toHZ2o-Ljwwm",
        "QMbSmc52n-Ni",
        "hrXQ2d7OsmLl"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
