{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "444rw1H7Tc5X"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DZZ-lQ6ZCSkw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdJi0t01bK4w"
      },
      "source": [
        "# I. Language detection (24 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLTloWrXUPjX"
      },
      "source": [
        "## Setup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvSPXjija_s3",
        "outputId": "b026fd5c-e454-4533-caa2-37f2a62d894c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LanguageDetection'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 11 (delta 2), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (11/11), 5.53 MiB | 5.76 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/MastafaF/LanguageDetection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1FzKr_KTn--",
        "outputId": "451e51b2-2093-4be2-c707-b0af5cbbfd32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.git', '.gitignore', 'dataset.csv.zip', 'LICENSE', 'README.md']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(\"./LanguageDetection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vUX_IVaqTrGY"
      },
      "outputs": [],
      "source": [
        "# CD the LanguageDetection folder - we are working in the below folder now\n",
        "os.chdir(\"./LanguageDetection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCV5bn74UAPv",
        "outputId": "71facc42-ae71-4f3f-b6fc-928c9d98fcab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  dataset.csv.zip\n",
            "  inflating: dataset.csv             \n"
          ]
        }
      ],
      "source": [
        "! unzip dataset.csv.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIBh8insUSc-"
      },
      "source": [
        "## Data Exploration Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xDfv7hhSUN6i"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"./dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "JAXFNxudyfTr",
        "outputId": "bdf1e93f-4a2e-4b5f-9742-f7eb24739c1b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18600</th>\n",
              "      <td>原子分子与光物理学專注於研究原子、分子與光，以及研究光与物质之間、物質与物质之間的相互作用。闡明物理的基礎定律、了解物質是怎樣在原子與分子層次組構而成、明白光與物質之間的相互作用、發展出新技術與新器件，這些是原子分子与光物理学的中心目標。原子分子与光物理学發展出的實驗與理論技術，時常會被應用於其它科學領域，例如，化學、天文物理學、生物學、醫藥學等等。對於很多其它科學領域，通過發展關於控制與操縱原子、分子與光的方法，或通過精確測量與分析它們的物理性質，或通過發展出新方法來製成具有某種特定性質的光，原子分子与光物理学扮演著賦能的角色。</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sample of the data\n",
        "HTML(data[data.language == \"Chinese\"].sample().to_html())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN0ADAdPVejT"
      },
      "source": [
        "### Question 1. Describe the distribution of languages and give at least two comments about the dataset. (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Fd1UdWiiVcoJ",
        "outputId": "d20b3d2f-3f0d-45b6-c145-4a607b38c7d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
              "      <td>Estonian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
              "      <td>Swedish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
              "      <td>Thai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
              "      <td>Tamil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
              "      <td>Dutch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21995</th>\n",
              "      <td>hors du terrain les années  et  sont des année...</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21996</th>\n",
              "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
              "      <td>Thai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21997</th>\n",
              "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
              "      <td>Spanish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21998</th>\n",
              "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21999</th>\n",
              "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
              "      <td>Romanian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  language\n",
              "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
              "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
              "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
              "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
              "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
              "...                                                  ...       ...\n",
              "21995  hors du terrain les années  et  sont des année...    French\n",
              "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
              "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
              "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
              "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
              "\n",
              "[22000 rows x 2 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Describe the distribution of the languages\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Q2tJlgz9wulS",
        "outputId": "447cb726-3150-4c72-9fbe-79e0889849ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'klement gottwaldi surnukeha palsameeriti ning paigutati mausoleumi surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke  aastal viidi ta surnukeha mausoleumist ära ja kremeeriti zlíni linn kandis aastatel – nime gottwaldov ukrainas harkivi oblastis kandis zmiivi linn aastatel – nime gotvald'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['Text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFMAub56wulS",
        "outputId": "b6331d03-8a3a-45f0-d30b-ac54a4f57b67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Estonian      1000\n",
              "Swedish       1000\n",
              "English       1000\n",
              "Russian       1000\n",
              "Romanian      1000\n",
              "Persian       1000\n",
              "Pushto        1000\n",
              "Spanish       1000\n",
              "Hindi         1000\n",
              "Korean        1000\n",
              "Chinese       1000\n",
              "French        1000\n",
              "Portugese     1000\n",
              "Indonesian    1000\n",
              "Urdu          1000\n",
              "Latin         1000\n",
              "Turkish       1000\n",
              "Japanese      1000\n",
              "Dutch         1000\n",
              "Tamil         1000\n",
              "Thai          1000\n",
              "Arabic        1000\n",
              "Name: language, dtype: int64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.language.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4QQAldXwulT"
      },
      "source": [
        "The languages are distributed evenly, with 1000 sentences for each language. The dataset is small, and the sentences are short. The dataset is balanced, with 1000 sentences for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdWOy0Naycz3"
      },
      "source": [
        "### Question 1-2. Do the appropriate pre-processing to maximise the accuracy of language detection. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A_KgR4U0N3_",
        "outputId": "c4bf78b9-1fcf-4585-a4df-8c8afe876779"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/leme/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "HZyVd2b9ymJr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<22000x264808 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 907817 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import word_tokenize, PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tokenize the text data\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess(data):\n",
        "    preprocessed_data = []\n",
        "    for text in data:\n",
        "        tokens = tokenize(text)\n",
        "        preprocessed_data.append(' '.join(tokens))\n",
        "    return preprocessed_data\n",
        "\n",
        "# Load the data\n",
        "sentences = data['Text']\n",
        "labels = data['language']\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, test_data, y_train, test_labels= train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data\n",
        "preprocessed_data = preprocess(sentences)\n",
        "# Represent the data as numerical features\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit_transform(preprocessed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<17600x264808 sparse matrix of type '<class 'numpy.int64'>'\n",
              " \twith 727363 stored elements in Compressed Sparse Row format>,\n",
              " <4400x264808 sparse matrix of type '<class 'numpy.int64'>'\n",
              " \twith 180454 stored elements in Compressed Sparse Row format>)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform train and test data to sparse matrix\n",
        "X_train = vectorizer.transform(preprocess(X_train))\n",
        "X_test = vectorizer.transform(preprocess(test_data))\n",
        "X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwab08eQ26UR"
      },
      "source": [
        "### Question 1-5. Train a model of your choice and describe the accuracy across languages. Use an 80%, 20% train-test split. Performance is not key but explain thoroughly the process and the metric(s) you are tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC_NPKqs2_Kx",
        "outputId": "0500ae9a-6b20-4db2-b625-927493c9b516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.96\n",
            "[[200   0   0   0   1   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0 101   0   3   0   0   0   0  97   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0 227   1   0   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   1   0 187   2   0   0   0   1   0   2   0   0   0   0   0   1   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0 192   1   0   1   3   0   1   0   0   0   0   2   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   0 185   0   0   1   0   0   0   0   0   0   0   2   0\n",
            "    0   0   0   0]\n",
            " [  0   1   0   2   0   0 205   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   1   0   0   0 212   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0  13   0   0   0   1   0   0 180   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   5 185   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   2   0   3   0   0   1   0 203   0   0   0   0   0   1   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   1   0   0   0   1   0   0 194   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   1   0   0   0 193   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   4   0   0   0   0   1   0   0   0   0 191   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0   2   0   1   0   0   0 193   0   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   1   0   0   0   0   4   0   0   0   0   0   0 208   0   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   2   0   0   0   0   1   0   0   0   1   0   0   0 195   0\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 179\n",
            "    0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  198   0   0   0]\n",
            " [  0   1   0   2   0   1   0   0   1   0   0   0   0   0   0   0   0   0\n",
            "    0 191   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
            "    0   0 198   0]\n",
            " [  0   0   0   3   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0 199]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearSVC()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the predictions\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Print a confusion matrix to see the performance of the model on each language\n",
        "confusion_matrix = confusion_matrix(test_labels, predictions)\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3jrbf7H4QoN",
        "outputId": "1d7343d6-5a31-41ff-989a-156c7dba4bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Estonian: 0.99\n",
            "Accuracy for Swedish: 0.50\n",
            "Accuracy for Thai: 0.99\n",
            "Accuracy for Tamil: 0.96\n",
            "Accuracy for Dutch: 0.96\n",
            "Accuracy for Japanese: 0.98\n",
            "Accuracy for Turkish: 0.99\n",
            "Accuracy for Latin: 1.00\n",
            "Accuracy for Urdu: 0.93\n",
            "Accuracy for Indonesian: 0.97\n",
            "Accuracy for Portugese: 0.97\n",
            "Accuracy for French: 0.99\n",
            "Accuracy for Chinese: 0.99\n",
            "Accuracy for Korean: 0.97\n",
            "Accuracy for Hindi: 0.98\n",
            "Accuracy for Spanish: 0.98\n",
            "Accuracy for Pushto: 0.98\n",
            "Accuracy for Persian: 1.00\n",
            "Accuracy for Romanian: 1.00\n",
            "Accuracy for Russian: 0.97\n",
            "Accuracy for English: 0.99\n",
            "Accuracy for Arabic: 0.98\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get the number of examples of each language in the test set\n",
        "num_examples_per_language = confusion_matrix.sum(axis=1)\n",
        "\n",
        "# Get the number of correctly classified examples of each language\n",
        "num_correct_per_language = np.diagonal(confusion_matrix)\n",
        "\n",
        "# Compute the accuracy of the model for each language\n",
        "accuracies_per_language = num_correct_per_language / num_examples_per_language\n",
        "\n",
        "languages = data['language'].unique().tolist()\n",
        "# Print the accuracy for each language\n",
        "for i, language in enumerate(languages):\n",
        "    print(f'Accuracy for {language}: {accuracies_per_language[i]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAnAmcxWJIdN"
      },
      "source": [
        "# II. Rotate two semantic spaces (23 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olI4GvHiKo6C"
      },
      "source": [
        "### Question 2-4. Align the French space and the English space together, with the method of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kkwdHXJKpQS",
        "outputId": "1b43c01d-ad22-4b83-c44a-0db39f9f7157"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity: 0.0001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-34-427db10de7af>:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  english_vectors = [english_embeddings[word] if word in english_embeddings else np.zeros(100) for sentence in parallel_data for word in sentence[0].split()]\n",
            "<ipython-input-34-427db10de7af>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  english_vectors = [english_embeddings[word] if word in english_embeddings else np.zeros(100) for sentence in parallel_data for word in sentence[0].split()]\n",
            "<ipython-input-34-427db10de7af>:15: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  french_vectors = [french_embeddings[word] if word in french_embeddings else np.zeros(100) for sentence in parallel_data for word in sentence[1].split()]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import numpy as np\n",
        "\n",
        "# Load the labeled parallel data\n",
        "parallel_data = [(\"this is a sentence in English\", \"c'est une phrase en français\"),\n",
        "                 (\"here is another sentence in English\", \"voici une autre phrase en français\")]\n",
        "\n",
        "# Learn the word embeddings for each language using word2vec\n",
        "english_embeddings = Word2Vec(sentences=[sentence for sentence, _ in parallel_data], size=100, window=5, min_count=1)\n",
        "french_embeddings = Word2Vec(sentences=[sentence for _, sentence in parallel_data], size=100, window=5, min_count=1)\n",
        "\n",
        "# Obtain the word embeddings for each unique word in the parallel data\n",
        "english_vectors = [english_embeddings[word] if word in english_embeddings else np.zeros(100) for sentence in parallel_data for word in sentence[0].split()]\n",
        "french_vectors = [french_embeddings[word] if word in french_embeddings else np.zeros(100) for sentence in parallel_data for word in sentence[1].split()]\n",
        "\n",
        "# Use the Procrustes alignment method to learn the mapping between the French and English word embeddings\n",
        "_, mapping = linear_sum_assignment(french_vectors, english_vectors)\n",
        "\n",
        "# Apply the mapping to transform the French word embeddings into the English space\n",
        "transformed_french_vectors = [english_vectors[i] for i in mapping]\n",
        "\n",
        "# Evaluate the quality of the alignment by comparing the transformed French vectors to the reference English vectors\n",
        "if len(english_vectors) > len(transformed_french_vectors):\n",
        "    similarity = sum(transformed_french_vectors[i] @ english_vectors[i] for i in range(len(transformed_french_vectors))) / len(transformed_french_vectors)\n",
        "else:\n",
        "    similarity = sum(transformed_french_vectors[i] @ english_vectors[i] for i in range(len(english_vectors))) / len(english_vectors)\n",
        "print(f\"Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk3qaAtfQbZZ"
      },
      "source": [
        "### Question 2-5. Visualize the output on a few words of your choice. Comment on the performance of the alignment based on the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "o9yE9_noQjde",
        "outputId": "2c853f69-a304-4f7f-da85-823e1a6761e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce the word embeddings to two dimensions using t-SNE\n",
        "english_reduced = TSNE(n_components=2).fit_transform(english_vectors)\n",
        "french_reduced = TSNE(n_components=2).fit_transform(transformed_french_vectors)\n",
        "\n",
        "# Create a scatter plot showing the French and English word embeddings\n",
        "plt.scatter(english_reduced[:, 0], english_reduced[:, 1], label=\"English\")\n",
        "plt.scatter(french_reduced[:, 0], french_reduced[:, 1], label=\"French\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G_S5NElSJNe"
      },
      "source": [
        "### Question 2-7. Apply your approach and comment on the performance of the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mFAce9sSJck"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czrUUAqJyoyW"
      },
      "source": [
        "### Question 2-5. Explore the data with your own preprocessing and train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "nNoW0UnlW4Bk",
        "outputId": "d6b7fc01-6e33-4bb8-faef-f9d59335096a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ab41778b31c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlm-roberta-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X=data['Text']\n",
        "y=data['language']\n",
        "\n",
        "# Split the data into train and test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"Text\"], truncation=True)\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({\"Text\": X_train, \"language\": y_train}))\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame({\"Text\": X_test, \"language\": y_test}))\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total # of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        ")\n",
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipeline = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    framework=\"pt\",\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0614tVO--6G"
      },
      "source": [
        "## FastText for language detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhGvhp6kf8E5"
      },
      "source": [
        "## FastText training setup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZtDxw9D9aPh",
        "outputId": "6d78d85a-dffa-419d-85c1-82375d4de4cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-11 17:53:02--  http://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
            "Resolving downloads.tatoeba.org (downloads.tatoeba.org)... 64:ff9b::5e82:4dc2, 94.130.77.194\n",
            "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|64:ff9b::5e82:4dc2|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.tatoeba.org/exports/sentences.tar.bz2 [following]\n",
            "--2023-01-11 17:53:02--  https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
            "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|64:ff9b::5e82:4dc2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 172915061 (165M) [application/octet-stream]\n",
            "Saving to: ‘sentences.tar.bz2’\n",
            "\n",
            "sentences.tar.bz2   100%[===================>] 164.90M  18.5MB/s    in 7.8s    \n",
            "\n",
            "2023-01-11 17:53:10 (21.3 MB/s) - ‘sentences.tar.bz2’ saved [172915061/172915061]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget http://downloads.tatoeba.org/exports/sentences.tar.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t9ETznJA9cLw"
      },
      "outputs": [],
      "source": [
        "! bunzip2 sentences.tar.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cihDm5c99oZh",
        "outputId": "32f4546f-fe08-49e8-8ed3-53f86bfc740f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LICENSE  README.md  dataset.csv  dataset.csv.zip  sentences.tar\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPZSloMN-cM0",
        "outputId": "7d57a6c4-3d01-4835-d5f9-906d8270bb52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentences.csv\n"
          ]
        }
      ],
      "source": [
        "! tar xvf sentences.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si8T701N-h5p",
        "outputId": "b6913132-02ae-40fc-fd2f-2a43c8328c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LICENSE  README.md  dataset.csv  dataset.csv.zip  sentences.csv  sentences.tar\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtjL8hYg-nZJ",
        "outputId": "bfa7c0d1-938f-4206-f2b0-1177b356a091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\tcmn\t我們試試看！\n",
            "2\tcmn\t我该去睡觉了。\n",
            "3\tcmn\t你在干什麼啊？\n",
            "4\tcmn\t這是什麼啊？\n",
            "5\tcmn\t今天是６月１８号，也是Muiriel的生日！\n",
            "6\tcmn\t生日快乐，Muiriel！\n",
            "7\tcmn\tMuiriel现在20岁了。\n",
            "8\tcmn\t密码是\"Muiriel\"。\n",
            "9\tcmn\t我很快就會回來。\n",
            "10\tcmn\t我不知道。\n"
          ]
        }
      ],
      "source": [
        "! head -10 sentences.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KTtbtb5E-tFP"
      },
      "outputs": [],
      "source": [
        "! awk -F\"\\t\" '{print\"__label__\"$2\" \"$3}' < sentences.csv | shuf > all.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5CwoZkl_JS2",
        "outputId": "74385dd1-4f84-48dc-8137-02479d10dbed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LICENSE    all.txt\tdataset.csv.zip  sentences.tar\n",
            "README.md  dataset.csv\tsentences.csv\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj17b-zF_V65",
        "outputId": "0caf6c25-5947-42c8-a99a-103482e48168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__label__heb אל תדאגו. לא אספר לאף אחד.\n",
            "__label__epo Ŝi gardas ŝian maljunan patrinon.\n",
            "__label__deu Wenn du etwas so machst, wie du es seit zehn Jahren gemacht hast, dann sind die Chancen groß, dass du es falsch machst.\n",
            "__label__ita Il cancro può essere guarito se riconosciuto in tempo.\n",
            "__label__eng Tom is the shortest boy in our class.\n"
          ]
        }
      ],
      "source": [
        "! head -5 all.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eJFTaHZ8_D6V"
      },
      "outputs": [],
      "source": [
        "! head -n 10000 all.txt > valid.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FJeo9_MD_qq8"
      },
      "outputs": [],
      "source": [
        "! tail -n +10001 all.txt > train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZSiMBU7Aabp",
        "outputId": "7826a4ac-cfd7-44b4-b9e4-1a57de568d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__label__tok meli sama mi li jo e ilo nena pi kalama musi. ona li pona mute.\n",
            "__label__eng We're in the middle of something here.\n",
            "__label__tur İyi bir anne olmak sana göre ne anlama geliyor?\n",
            "__label__fin Kaivossa ei ollut vettä.\n",
            "__label__fra L'humanité va disparaître.\n"
          ]
        }
      ],
      "source": [
        "! head -5 train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YN516w-3Mea"
      },
      "source": [
        "### Question 6.1. Train fasttext model on Tatoeba (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpaHpA9QCLur",
        "outputId": "cda439d9-7b90-4855-db22-bd66d1013265"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Read 89M words\n",
            "Number of words:  4178903\n",
            "Number of labels: 415\n",
            "Progress: 100.0% words/sec/thread:   45780 lr:  0.000000 avg.loss:  0.132235 ETA:   0h 0m 0s% words/sec/thread:   49266 lr:  0.048142 avg.loss:  0.207991 ETA:   0h 6m37s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2h 32min 24s, sys: 26.4 s, total: 2h 32min 50s\n",
            "Wall time: 15min 33s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import fasttext\n",
        "\n",
        "# Check the fasttext library and implement the training\n",
        "###########################################\n",
        "\n",
        "# Load the training data\n",
        "train_data = \"train.txt\"\n",
        "valid_data = \"valid.txt\"\n",
        "\n",
        "# Train the model\n",
        "model = fasttext.train_supervised(input=train_data)\n",
        "\n",
        "################################################\n",
        "\n",
        "# @TODO: Save your model when trained \n",
        "model.save_model(\"langdetect.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFQcolkmIXZr",
        "outputId": "dac3f9ac-721f-4512-d8c1-89d2285a2083"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('__label__eng',), array([1.00000906]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sanity check \n",
        "model.predict(\"I am French and I love English\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZYQz3zh3VK5"
      },
      "source": [
        "### Question 6.2. Evaluate performance of fasttext model on valid.txt (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "model = fasttext.load_model(\"langdetect.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "SWNZ5TSdKGgT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.9569, Recall: 0.9569\n"
          ]
        }
      ],
      "source": [
        "# Hint: Create dataframe from valid.txt and evaluate performance \n",
        "\n",
        "###########################################\n",
        "\n",
        "# Evaluate the model on valid data\n",
        "nb_samples, precision, recall = model.test(valid_data)\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KssPimX_lbNS"
      },
      "source": [
        "### Question 7 & 8. Test your FastText model on the same dataset as question 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform test data to fastttext format\n",
        "def transform_to_fasttext_format(data, labels, filename=\"test.txt\"):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for index in data.index:\n",
        "            f.write(f\"__label__{labels[index][:3].lower()} {data[index]}\\n\")\n",
        "\n",
        "transform_to_fasttext_format(test_data, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "oJYVrn8FhcQf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.7579, Recall: 0.7579\n"
          ]
        }
      ],
      "source": [
        "###########################################\n",
        "\n",
        "nb_samples, precision, recall = model.test(\"test.txt\")\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "7sgOYS7uhcB5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('__label__jpn',), array([0.88705397]))"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
