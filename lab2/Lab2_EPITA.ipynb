{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "444rw1H7Tc5X"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DZZ-lQ6ZCSkw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdJi0t01bK4w"
      },
      "source": [
        "# I. Language detection (24 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLTloWrXUPjX"
      },
      "source": [
        "## Setup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvSPXjija_s3",
        "outputId": "3a9ffe46-a98c-499c-b396-90f5d4c455d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'LanguageDetection' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/MastafaF/LanguageDetection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1FzKr_KTn--",
        "outputId": "4509f837-a6bb-412a-8662-0a67c1ff8b9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['results',\n",
              " 'dataset.csv.zip',\n",
              " 'LICENSE',\n",
              " 'README.md',\n",
              " 'dataset.csv',\n",
              " '.git',\n",
              " '.gitignore',\n",
              " 'logs']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(\"./LanguageDetection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vUX_IVaqTrGY"
      },
      "outputs": [],
      "source": [
        "# CD the LanguageDetection folder - we are working in the below folder now\n",
        "os.chdir(\"./LanguageDetection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCV5bn74UAPv",
        "outputId": "61a0ad31-4b11-486c-d36e-de2a70130046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  dataset.csv.zip\n",
            "replace dataset.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ]
        }
      ],
      "source": [
        "! unzip dataset.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIBh8insUSc-"
      },
      "source": [
        "## Data Exploration Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xDfv7hhSUN6i"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"./dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "JAXFNxudyfTr",
        "outputId": "a3b07aac-4508-418a-ae03-e565dd07d908"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21310</th>\n",
              "      <td>基斯·保殊的昵称是cb，cb是他英语的首字母缩写是暴龍時期球衣背号，第一个这么称呼他的是猛龙队解说员chuck swirsky。 赛场下的波什经常参加慈善活动和社区公益服务，由于自己童年的经历，他创建了基斯·保殊慈善基金会，协助提高达拉斯和多伦多青少年体育的教育事业，有规律的向青少年讲述阅读的好处。年休賽期中，邁阿密熱火隊宣佈裁掉克里斯·波什，結束年效力熱火的生涯。 但他尚未選擇退休。</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sample of the data\n",
        "HTML(data[data.language == \"Chinese\"].sample().to_html())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN0ADAdPVejT"
      },
      "source": [
        "### Question 1. Describe the distribution of languages and give at least two comments about the dataset. (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Fd1UdWiiVcoJ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
              "      <td>Estonian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
              "      <td>Swedish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
              "      <td>Thai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
              "      <td>Tamil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
              "      <td>Dutch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21995</th>\n",
              "      <td>hors du terrain les années  et  sont des année...</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21996</th>\n",
              "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
              "      <td>Thai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21997</th>\n",
              "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
              "      <td>Spanish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21998</th>\n",
              "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21999</th>\n",
              "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
              "      <td>Romanian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  language\n",
              "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
              "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
              "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
              "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
              "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
              "...                                                  ...       ...\n",
              "21995  hors du terrain les années  et  sont des année...    French\n",
              "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
              "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
              "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
              "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
              "\n",
              "[22000 rows x 2 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Describe the distribution of the languages\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'klement gottwaldi surnukeha palsameeriti ning paigutati mausoleumi surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke  aastal viidi ta surnukeha mausoleumist ära ja kremeeriti zlíni linn kandis aastatel – nime gottwaldov ukrainas harkivi oblastis kandis zmiivi linn aastatel – nime gotvald'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['Text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Estonian      1000\n",
              "Swedish       1000\n",
              "English       1000\n",
              "Russian       1000\n",
              "Romanian      1000\n",
              "Persian       1000\n",
              "Pushto        1000\n",
              "Spanish       1000\n",
              "Hindi         1000\n",
              "Korean        1000\n",
              "Chinese       1000\n",
              "French        1000\n",
              "Portugese     1000\n",
              "Indonesian    1000\n",
              "Urdu          1000\n",
              "Latin         1000\n",
              "Turkish       1000\n",
              "Japanese      1000\n",
              "Dutch         1000\n",
              "Tamil         1000\n",
              "Thai          1000\n",
              "Arabic        1000\n",
              "Name: language, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.language.value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The languages are distributed evenly, with 1000 sentences for each language. The dataset is small, and the sentences are short. The dataset is balanced, with 1000 sentences for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czrUUAqJyoyW"
      },
      "source": [
        "### Question 2-5. Explore the data with your own preprocessing and train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nNoW0UnlW4Bk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading:  47%|████▋     | 4.31M/9.10M [00:08<00:04, 1.07MB/s]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/bictole/Documents/NLP3/lab2/Lab2_EPITA.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bictole/Documents/NLP3/lab2/Lab2_EPITA.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bictole/Documents/NLP3/lab2/Lab2_EPITA.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bictole/Documents/NLP3/lab2/Lab2_EPITA.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mxlm-roberta-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bictole/Documents/NLP3/lab2/Lab2_EPITA.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mxlm-roberta-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bictole/Documents/NLP3/lab2/Lab2_EPITA.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_function\u001b[39m(examples):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:628\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 628\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    629\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1734\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m download_url(file_path, proxies\u001b[39m=\u001b[39mproxies)\n\u001b[1;32m   1733\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1735\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1736\u001b[0m             file_path,\n\u001b[1;32m   1737\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1738\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1739\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1740\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1741\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1742\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1743\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1744\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1745\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1746\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1747\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1748\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1749\u001b[0m         )\n\u001b[1;32m   1750\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1226\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1224\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1226\u001b[0m     http_get(\n\u001b[1;32m   1227\u001b[0m         url_to_download,\n\u001b[1;32m   1228\u001b[0m         temp_file,\n\u001b[1;32m   1229\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1230\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1231\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1232\u001b[0m     )\n\u001b[1;32m   1234\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[1;32m   1235\u001b[0m os\u001b[39m.\u001b[39mreplace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:490\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    481\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    482\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    483\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    484\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    489\u001b[0m )\n\u001b[0;32m--> 490\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
            "File \u001b[0;32m/usr/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/response.py:579\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 579\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    582\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/response.py:522\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     cache_content \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    523\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    524\u001b[0m         amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data\n\u001b[1;32m    525\u001b[0m     ):  \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    532\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m/usr/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading:  48%|████▊     | 4.41M/9.10M [00:19<00:04, 1.07MB/s]"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X=data['Text']\n",
        "y=data['language']\n",
        "\n",
        "# Split the data into train and test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"Text\"], truncation=True)\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({\"Text\": X_train, \"language\": y_train}))\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame({\"Text\": X_test, \"language\": y_test}))\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total # of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        ")\n",
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipeline = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    framework=\"pt\",\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0614tVO--6G"
      },
      "source": [
        "## FastText for language detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhGvhp6kf8E5"
      },
      "source": [
        "## FastText training setup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZtDxw9D9aPh",
        "outputId": "6d78d85a-dffa-419d-85c1-82375d4de4cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-11-01 14:44:47--  http://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
            "Resolving downloads.tatoeba.org (downloads.tatoeba.org)... 94.130.77.194\n",
            "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|94.130.77.194|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.tatoeba.org/exports/sentences.tar.bz2 [following]\n",
            "--2021-11-01 14:44:47--  https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
            "Connecting to downloads.tatoeba.org (downloads.tatoeba.org)|94.130.77.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 155856043 (149M) [application/octet-stream]\n",
            "Saving to: ‘sentences.tar.bz2’\n",
            "\n",
            "sentences.tar.bz2   100%[===================>] 148.64M  28.4MB/s    in 5.9s    \n",
            "\n",
            "2021-11-01 14:44:54 (25.4 MB/s) - ‘sentences.tar.bz2’ saved [155856043/155856043]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget http://downloads.tatoeba.org/exports/sentences.tar.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9ETznJA9cLw"
      },
      "outputs": [],
      "source": [
        "! bunzip2 sentences.tar.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cihDm5c99oZh",
        "outputId": "32f4546f-fe08-49e8-8ed3-53f86bfc740f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset.csv  dataset.csv.zip  LICENSE  README.md  sentences.tar\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPZSloMN-cM0",
        "outputId": "7d57a6c4-3d01-4835-d5f9-906d8270bb52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentences.csv\n"
          ]
        }
      ],
      "source": [
        "! tar xvf sentences.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si8T701N-h5p",
        "outputId": "b6913132-02ae-40fc-fd2f-2a43c8328c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset.csv  dataset.csv.zip  LICENSE  README.md  sentences.csv  sentences.tar\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtjL8hYg-nZJ",
        "outputId": "bfa7c0d1-938f-4206-f2b0-1177b356a091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\tcmn\t我們試試看！\n",
            "2\tcmn\t我该去睡觉了。\n",
            "3\tcmn\t你在干什麼啊？\n",
            "4\tcmn\t這是什麼啊？\n",
            "5\tcmn\t今天是６月１８号，也是Muiriel的生日！\n",
            "6\tcmn\t生日快乐，Muiriel！\n",
            "7\tcmn\tMuiriel现在20岁了。\n",
            "8\tcmn\t密码是\"Muiriel\"。\n",
            "9\tcmn\t我很快就會回來。\n",
            "10\tcmn\t我不知道。\n"
          ]
        }
      ],
      "source": [
        "! head -10 sentences.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTtbtb5E-tFP"
      },
      "outputs": [],
      "source": [
        "! awk -F\"\\t\" '{print\"__label__\"$2\" \"$3}' < sentences.csv | shuf > all.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5CwoZkl_JS2",
        "outputId": "74385dd1-4f84-48dc-8137-02479d10dbed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all.txt      dataset.csv.zip  README.md      sentences.tar\n",
            "dataset.csv  LICENSE\t      sentences.csv\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj17b-zF_V65",
        "outputId": "0caf6c25-5947-42c8-a99a-103482e48168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__label__ber Ur bɣint ara ad d-aɣent asegzawal tafṛansit-takatalanit.\n",
            "__label__epo Tom deziras riĉiĝi kaj famiĝi.\n",
            "__label__deu Tom sagte, das sei bei weitem nicht gut genug.\n",
            "__label__kab Amek i ilaq ara ad yekteb talɣa s tendunizit?\n",
            "__label__epo Ĉu ni havas raspitan fromaĝon?\n"
          ]
        }
      ],
      "source": [
        "! head -5 all.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJFTaHZ8_D6V"
      },
      "outputs": [],
      "source": [
        "! head -n 10000 all.txt > valid.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJeo9_MD_qq8"
      },
      "outputs": [],
      "source": [
        "! tail -n +10001 all.txt > train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZSiMBU7Aabp",
        "outputId": "7826a4ac-cfd7-44b4-b9e4-1a57de568d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__label__asm তাৰ পইচাৰ দৰকাৰ হৈছে।\n",
            "__label__tur O, bir dolap çeviriyor.\n",
            "__label__tur Bazı genç kadınlar kötü oğlanları cezbediyor.\n",
            "__label__por O Japão possui muitas atrações turísticas.\n",
            "__label__ukr Я довідалася про Ваш портативний копір на Tokyo Office Expo 97.\n"
          ]
        }
      ],
      "source": [
        "! head -5 train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YN516w-3Mea"
      },
      "source": [
        "### Question 6.1. Train fasttext model on Tatoeba (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpaHpA9QCLur",
        "outputId": "cda439d9-7b90-4855-db22-bd66d1013265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 12 µs, sys: 0 ns, total: 12 µs\n",
            "Wall time: 15.3 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import fasttext\n",
        "\n",
        "# Check the fasttext library and implement the training\n",
        "###########################################\n",
        "\n",
        "# your implementation goes here\n",
        "\n",
        "pass\n",
        "################################################\n",
        "\n",
        "# @TODO: Save your model when trained \n",
        "# model.save_model(\"langdetect.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFQcolkmIXZr",
        "outputId": "dac3f9ac-721f-4512-d8c1-89d2285a2083"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('__label__eng',), array([1.00001001]))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sanity check \n",
        "model.predict(\"I am French and I love English\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZYQz3zh3VK5"
      },
      "source": [
        "### Question 6.2. Evaluate performance of fasttext model on valid.txt (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWNZ5TSdKGgT"
      },
      "outputs": [],
      "source": [
        "# Hint: Create dataframe from valid.txt and evaluate performance \n",
        "\n",
        "###########################################\n",
        "\n",
        "# your implementation goes here\n",
        "\n",
        "pass\n",
        "################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KssPimX_lbNS"
      },
      "source": [
        "### Question 7 & 8. Test your FastText model on the same dataset as question 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJYVrn8FhcQf"
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "\n",
        "# your implementation goes here\n",
        "\n",
        "pass\n",
        "################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sgOYS7uhcB5"
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "\n",
        "# your implementation goes here\n",
        "\n",
        "pass\n",
        "################################################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSEb+xrP/z9eK9EbHeWaJj",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Lab2_EPITA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "usr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
